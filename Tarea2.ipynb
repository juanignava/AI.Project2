{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instituto Tecnológico de Costa Rica (ITCR)\n",
    "### Escuela de Computación\n",
    "### Curso: Inteligencia Artificial\n",
    " \n",
    "### Segunda tarea programada 2022-I\n",
    "\n",
    "\n",
    "Estudiantes: Juan Ignacio Navarro Navarro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 1. Graficación\n",
    "\n",
    "---\n",
    "\n",
    "Para las siguientes:\n",
    "\n",
    "Función 1:\n",
    "\n",
    "$f_{1}\\left(x_{1},x_{2}\\right)=\\left(x_{1}-0.7\\right)^{2}+\\left(x_{2}-0.5\\right)^{2}$\n",
    "\n",
    "\n",
    "Función 2: \n",
    "\n",
    "$f_{2}\\left(x_{1},x_{2}\\right)=x_{1}e^{\\left(-x_{1}^{2}-x_{2}^{2}\\right)}$\n",
    "\n",
    "\n",
    "Realice lo siguiente (5 puntos):\n",
    "\n",
    "En Python, cree una función que le permita graficar las funciones anteriores. Utilizando la función en Python genere dos gráficos, uno para cada función $𝑓_{1}$ y  $𝑓_{2}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import exp,arange\n",
    "from pylab import meshgrid,cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Se define la funcion que se va a graficar\n",
    "\n",
    "def func_1(x1,x2):\n",
    "    \"\"\"\n",
    "    Este método define el resultado de la función 1\n",
    "    del enunciado dados dos valores x1 y x2\n",
    "    \"\"\"\n",
    "    return (x1 - 0.7)**2 + (x2 - 0.5)**2\n",
    "\n",
    "\n",
    "def func_2(x1, x2):\n",
    "    \"\"\"\n",
    "    Este método define el resultado de la función 2\n",
    "    del enunciado dados dos valores x1 y x2\n",
    "    \"\"\"\n",
    "    return x1 * exp(-x1**2 - x2**2)\n",
    "\n",
    "def grafica3D(X,Y,Z):\n",
    "    \"\"\"\n",
    "    Esta fución realiza la gráfica una función con dos variables\n",
    "    independientes X, Y y una dependiente Z\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig)\n",
    "\n",
    "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.RdBu,linewidth=0, antialiased=False)\n",
    "\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Definición del rango de valores a graficar\n",
    "x = arange(-2.0,2.0,0.1)\n",
    "y = arange(-2.0,2.0,0.1)\n",
    "\n",
    "#Se define la grilla de puntos para x y y\n",
    "X,Y = meshgrid(x, y)\n",
    "\n",
    "# Se evalua la primera funcion 1 segun los valores de X y Y\n",
    "Z = func_1(X, Y)\n",
    "\n",
    "# Se grafica la primera función 1\n",
    "grafica3D(X,Y,Z)\n",
    "\n",
    "# Se evalua la segunda funcion 2 con los valores de X y Y \n",
    "Z = func_2(X, Y)\n",
    "\n",
    "# Se grafica la primera funcion 2\n",
    "grafica3D(X, Y, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 2. Optimización\n",
    "---\n",
    "\n",
    "**a) Vector gradiente  (5 puntos)**\n",
    "\n",
    "Calcule el vector gradiente $\\nabla f$ para la siguiente función multi-variable $f: \\mathbb{R}^2\\rightarrow\\mathbb{R}$. Además, evalúelo en $\\begin{bmatrix}1\\\\1\\end{bmatrix}$ y $\\begin{bmatrix}-1\\\\-1\\end{bmatrix}$. \n",
    "\n",
    "--No se require programación en python--.\n",
    "\n",
    " - $f_3(x,y) = x^4+y^3+5x^2y^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuesta\n",
    "\n",
    "El gradiente de la función $f_3$ es:\n",
    "\n",
    "$$\\nabla _{f_3}(x, y) = (4x^3 + 10xy^3, 3y^2+ 15x^2y^2)$$\n",
    "\n",
    "Al evaluar el vector en $\\begin{bmatrix}1\\\\1\\end{bmatrix}$ se obtiene:\n",
    "\n",
    "$$\\nabla _{f_3}(1, 1) = (4 + 10, 3 + 15)$$\n",
    "$$\\nabla _{f_3}(1, 1) = (14, 18)$$\n",
    "\n",
    "Ahora al evaluarlo en $\\begin{bmatrix}-1\\\\-1\\end{bmatrix}$:\n",
    "\n",
    "$$\\nabla _{f_3}(-1, -1) = (-4 + 10, 3 + 15)$$\n",
    "$$\\nabla _{f_3}(-1, -1) = (6, 18)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Descenso de gradiente (10 puntos)**\n",
    "\n",
    "**Sea la función:** \n",
    "\n",
    "\\begin{equation}\n",
    "f_4\\left(\\overrightarrow{x}\\right)=(x-0.7)^{2}+(y-0.5)^{2},\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Implemente el algoritmo de descenso de gradiente para $f_4(x)$ en la función en Python denominada: (8 puntos)**\n",
    "\n",
    "$$funcion\\_gradient\\_descent \\left(learning\\_rate, max\\_iters, starting\\_point, f\\_function,f\\_gradient, precision\\right)$$\n",
    "\n",
    "donde los parámetros corresponden a:\n",
    "\n",
    "* learning_rate: tasa_aprendizaje o el $\\alpha$\n",
    "* max_iters: es el máximo número de iteraciones a ejecutar\n",
    "* starting_point: es el vector con los dos valores iniciales [x,y]\n",
    "* f_function: fución a optimizar\n",
    "* f_gradient: gradiente de la función a optimizar.\n",
    "* precision: es el valor mínimo para un cambio entre iteración. \n",
    "\n",
    "Despliegue y analice los resultados (2 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_4(x, y):\n",
    "    \"\"\"\n",
    "    Codigo de la función 4 definida en el enunciado\n",
    "    \"\"\"\n",
    "    return (x - 0.7)^2 + (y - 0.5)^2\n",
    "\n",
    "def gfunc_4(x, y):\n",
    "    \"\"\"\n",
    "    Gradiente de la función 4 definida en el enunciado\n",
    "    \"\"\"\n",
    "    return 2*(x - 0.7), 2*(y - 0.5)\n",
    "\n",
    "def function_gradient_descent(learning_rate, max_iters, starting_point, f_function, f_gradient, precision):\n",
    "    \"\"\"\n",
    "    Esta función calcula el restultado del algoritmo de descenso de gradiente\n",
    "    \"\"\"\n",
    "    current_value = starting_point\n",
    "    grad_norm = 1\n",
    "    iters = 0\n",
    "\n",
    "    # iteracion hasta que la noram del gradiente sea menos a la precision indicada\n",
    "    while grad_norm > precision and iters < max_iters:\n",
    "        curr_x = current_value[0]\n",
    "        curr_y = current_value[1]\n",
    "        curr_grad = f_gradient(curr_x, curr_y)\n",
    "        prev_value = current_value\n",
    "        # calculo del nuevo valor: x = x - k * gradiente( f(x) )\n",
    "        current_value = prev_value[0] - learning_rate * curr_grad[0] , prev_value[1] - learning_rate * curr_grad[1]\n",
    "        # actualización de la norma del gradiente\n",
    "        grad_norm = (curr_grad[0]**2 + curr_grad[1]**2)**0.5\n",
    "        iters += 1\n",
    "\n",
    "    return current_value\n",
    "\n",
    "# definicion de las variables iniciales\n",
    "learning_rate = 0.01\n",
    "max_iters = 10000\n",
    "starting_point = (3, 2)\n",
    "f_function = func_4\n",
    "f_gradient = gfunc_4\n",
    "precision = 0.000001\n",
    "\n",
    "# como resultado se imprime el valor del descenso de gradiente\n",
    "print(function_gradient_descent(learning_rate, max_iters, starting_point, f_function, f_gradient, precision))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 3. Análisis de regresión con aprendizaje profundo (con PyTorch)\n",
    "\n",
    "Para realizar el análisis se utilizará un conjunto de datos generado por la Facultad de Ciencias de la Información y la Computación Donald Bren de la Universidad de California en Irvine disponibles en https://www.kaggle.com/rodolfomendes/abalone-dataset (copia adjunta).   \n",
    "\n",
    "Descripción de los datos:\n",
    "El conjunto de datos puede ser utilizado para entrenar modelos para predecir la edad de los abulones (moluscos también conocidos como orejas de mar) a partir de mediciones físicas. Comúnmente, la edad de un abulón se determina cortando la concha a través del cono, tiñéndola y contando el número de anillos a través de un microscópio, una tarea que requiere mucho tiempo. Sin embargo, es posible utilizar datos morfológicos del individuo, que son más fáciles de obtener y permiten predecir la edad este. Una descripción detallada de los datos está disponible en https://archive.ics.uci.edu/ml/datasets/abalone. \n",
    "\n",
    "Dado el conjunto de datos de abulones se **desea crear un modelo de regresión utilizando un perceptrón multicapa para predecir la cantidad de anillos (columna Rings) de estos a partir del conjunto de características**.\n",
    "\n",
    "Realice lo siguiente:\n",
    "\n",
    "(requisito indispensable para tomar en cuenta el ejercicio, deben usar PyTorch).\n",
    "- a) (1 punto) Describa el conjunto de datos, cada uno de sus campos y referencie la fuente. \n",
    "- b) Cargue el conjunto de datos.\n",
    "- c) (3 puntos) Explore y limpie el conjunto de datos, visualice algunas estadísticas, presente una matriz de calor y verifique que no existan valores faltantes.\n",
    "- d) (5 puntos) Defina el modelo utilizando un perceptrón multicapa implementado con PyTorch.\n",
    "- e) (5 puntos) Realice el entrenamiento del modelo.\n",
    "- f) (5 puntos) Calcule la pérdida utilizando el error cuadrático medio.\n",
    "- g) (2 puntos) Prepare un ejemplo de uso del modelo seleccionado y explique el resultado.\n",
    "- h) (5 puntos) Realice al menos tres conclusiones sobre el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)\n",
    "\n",
    "Descripción de los datos:\n",
    "\n",
    "- Lenght: corresponde a la longitud más larga de la concha en milímetros.\n",
    "- Diameter: al diametro en milimetros perpendicular a la longitud.\n",
    "- Height: Es la altura en milimetros con carne en la concha.\n",
    "- Whole: Es el peso total en gramos de todo el abalón.\n",
    "- Viscera: Es el peso en gramos de la carne.\n",
    "- Shellweight: Es el peso de la concha en gramos después de ser secada.\n",
    "- Rings: es un entero que corresponde a la cantidad de anillos. Es 1.5 más en adición a la cantidad de años del abalón.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "- Carga de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas requeridas para el ejercicio\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch as torch\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de los datos de los abalones\n",
    "\n",
    "data = pd.read_csv('datos/Abalone.csv')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c)\n",
    "- Exploración de los datos y mapa de calor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribución de los anillos con respecto a la longitud\n",
    "\n",
    "def plot_data(df, col_x, col_y, label_x, label_y, val_title):\n",
    "    # Imprime gráfica de dispersión. \n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.scatter(x=df[col_x],y=df[col_y])\n",
    "    plt.xlabel(label_x)\n",
    "    plt.ylabel(label_y)\n",
    "    plt.title(val_title)\n",
    "    plt.show()\n",
    "    \n",
    "plot_data(data,'Length', 'Rings', 'Longitud de la concha','Anillos',\n",
    "          'Longitud de la concha - Relación anillos')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note que hay datos que están un poco perdidos por lo que se debe\n",
    "#  realizar una limpieza en el modelo, esto se hace a continuación\n",
    "\n",
    "data = data[data.Rings <= 100]\n",
    "data = data[data.Rings >= -100]\n",
    "\n",
    "plot_data(data,'Length', 'Rings', 'Longitud de la concha','Anillos',\n",
    "          'Longitud de la concha - Relación anillos')\n",
    "\n",
    "# Note que al correr el modelo ahora los datos se ven mejor \n",
    "#  sin el ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribución de los anillos con respecto al diametro\n",
    "    \n",
    "plot_data(data,'Diameter', 'Rings', 'Diametro de la concha','Anillos',\n",
    "          'Diametro de la concha - Relación anillos')\n",
    "\n",
    "# En este caso no se observan valores perdidos por lo que no hace\n",
    "# falta la limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribución de los anillos con respecto a la altura\n",
    "    \n",
    "plot_data(data,'Height', 'Rings', 'Altura de la concha','Anillos',\n",
    "          'Altura de la concha - Relación anillos')\n",
    "\n",
    "# En este caso no se observan valores perdidos por lo que no hace\n",
    "# falta la limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribución de los anillos con respecto al peso completo\n",
    "    \n",
    "plot_data(data,'Whole', 'Rings', 'Peso de todo el abalón','Anillos',\n",
    "          'Peso de todo el abalón - Relación anillos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este caso se podría hacer una pequeña limpieza para apreciar\n",
    "# mejor los valores acomulados\n",
    "\n",
    "data = data[data.Whole <= 2]\n",
    "\n",
    "plot_data(data,'Whole', 'Rings', 'Peso de todo el abalón','Anillos',\n",
    "          'Peso de todo el abalón - Relación anillos')\n",
    "\n",
    "# Note que ahora se ve una mejor distribución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribución de los anillos con respecto al peso de la viscera\n",
    "# del abalón\n",
    "    \n",
    "plot_data(data,'Viscera', 'Rings', 'Peso de la viscera del abalón','Anillos',\n",
    "          'Peso de la viscera del abalón - Relación anillos')\n",
    "\n",
    "# aprovechamos para realizar un pequeño corte\n",
    "\n",
    "data = data[data.Viscera <= 0.4]\n",
    "\n",
    "plot_data(data,'Whole', 'Rings', 'Peso de todo el abalón','Anillos',\n",
    "          'Peso de todo el abalón - Relación anillos')\n",
    "\n",
    "# Ahora se ve un poco mejor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribución de los anillos con respecto al peso de la concha\n",
    "# del abalón\n",
    "    \n",
    "plot_data(data,'Shellweight', 'Rings', 'Peso de la concha del abalón','Anillos',\n",
    "          'Peso de la concha del abalón - Relación anillos')\n",
    "\n",
    "# aprovechamos para realizar un pequeño corte\n",
    "\n",
    "data = data[data.Shellweight <= 0.5]\n",
    "\n",
    "plot_data(data,'Shellweight', 'Rings', 'Peso de la concha del abalón','Anillos',\n",
    "          'Peso de la concha del abalón - Relación anillos')\n",
    "\n",
    "# Ahora se ve un poco mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadíticas de los datos\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La siguiente gráfica presenta relación entre pares de variables.\n",
    "\n",
    "# En la diagonal se ven los histogramas de cada variables\n",
    "# La segunda fila presenta la ultima fila presenta la relación de la variable Anillos\n",
    "# con todas las demás\n",
    "\n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación de los datos\n",
    "\n",
    "# A continuación se muestra la correlación de los datos por medio del mapa de calor\n",
    "\n",
    "# Se puede observar que los datos están muy relacionados entre sí a excepción de \n",
    "# las relaciones con los anillos. \n",
    "\n",
    "def correlation_heatmap(df1):\n",
    "    _,ax=plt.subplots(figsize=(15,10))\n",
    "    colormap=sns.diverging_palette(220,10,as_cmap=True)\n",
    "    sns.heatmap(data.corr(),annot=True,cmap=colormap)\n",
    "    \n",
    "correlation_heatmap(data)\n",
    "\n",
    "# Note que en la mayoría los valores están altamente correlacionados\n",
    "# En realidad la última columna/fila (tiene los mismos valores) \n",
    "# de esta matriz es la que más nos interesa, es la correlación \n",
    "# de todas las gráficas que vimos anteriormente. Note que las variables\n",
    "# que tienen más correlación serán las que probablemente tengan \n",
    "# más peso en el modelo de predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) \n",
    "Defina el modelo utilizando un perceptrón multicapa implementado con PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principales bibliotecas que utilizarán en el perceptrón\n",
    "from numpy import vstack\n",
    "from numpy import sqrt\n",
    "from numpy import array\n",
    "from pandas import read_csv, notnull\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.init import xavier_uniform_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo corresponde al siguiente código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES\n",
    "\n",
    "# definicion del dataset\n",
    "class CSVDataset(Dataset):\n",
    "    # carga del dataset\n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        The __init__ function is run once when instantiating the Dataset object. \n",
    "        :param path: the path and name of the file to process. \n",
    "        \"\"\"\n",
    "        # load the csv file as a dataframe\n",
    "        df = pd.read_csv(path)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1].astype('float32')\n",
    "        self.y = df.values[:, -1].astype('float32')\n",
    "        # ensure target has the right shape\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The __len__ function returns the number of samples in our dataset.\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The __getitem__ function loads and returns \n",
    "        a sample from the dataset at the given index idx. \n",
    "        :param idx: index.\n",
    "        \"\"\"\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        \"\"\"\n",
    "        Split the dataset into training and test data.\n",
    "        :param n_test: training data percentage\n",
    "        \"\"\"\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "    \n",
    "def prepare_data(path):\n",
    "    \"\"\"\n",
    "    Prepare the dataset.\n",
    "    :param path: path and name of the file .\n",
    "    \"\"\"\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "\n",
    "def evaluate_model(test_dl, model):\n",
    "    \"\"\"\n",
    "    Evaluates the model performance using Mean Squared Error (MSE).\n",
    "    :param: test_dt: test data.\n",
    "    :param: model: model to evaluate.\n",
    "    \"\"\"\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate mse\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def predict(row, model):\n",
    "    \"\"\"\n",
    "    Make a class prediction for one row of data\n",
    "    \"\"\"\n",
    "\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class MLP(Module):\n",
    "    \"\"\"\n",
    "    Class that implements the perceptron, it extends the Module class.\n",
    "    \"\"\"\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs, n_output):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 4)\n",
    "        # Values are scaled by the gain parameter using a uniform distribution.\n",
    "        # No gradient will be recorded for this operation.\n",
    "        xavier_uniform_(self.hidden1.weight)\n",
    "        self.act1 = Sigmoid()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(4, 2)\n",
    "        xavier_uniform_(self.hidden2.weight)\n",
    "        self.act2 = Sigmoid()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(2, n_output)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward run of the network using the data in X.\n",
    "        \"\"\"\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "Realice el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, model):\n",
    "    \"\"\"\n",
    "    Train the model using the train data loader (train_dl).\n",
    "    \"\"\"\n",
    "    \n",
    "    # define the optimization\n",
    "    # Mean Squared Error (MSE)\n",
    "    criterion = MSELoss()\n",
    "    # Stochastic gradient descent (SGD)\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661 326\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001CB40859B80>\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "path = 'datos/Abalone.csv'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "print(train_dl)\n",
    "\n",
    "# define the network\n",
    "n_inputs = 6\n",
    "n_output = 1\n",
    "model = MLP( n_inputs, n_output)\n",
    "\n",
    "# train the model\n",
    "train_model(train_dl, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)\n",
    "\n",
    "Calcule la pérdida usando el error cuadrático medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 255593.797, RMSE: 505.563\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo usando el error cuadrático medio (MSE)\n",
    "mse = evaluate_model(test_dl, model)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))\n",
    "\n",
    "# Note que al correrlo se obtiene un error cuadrático medio bajo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g)\n",
    "\n",
    "Prepare un modelo del ejemplo determinado y explique los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción del modelo: 17.063\n",
      "Valor obtenido de los datos: 17\n"
     ]
    }
   ],
   "source": [
    "# Realizamos una predicción con el modelo con la primera\n",
    "#  fila de datos del documento del Abalón, cuyo valor\n",
    "#  esperado corresponde a 17 anillos\n",
    "row = [0.745,0.585,0.215,2.499,0.472,0.7]\n",
    "yhat = predict(row, model)\n",
    "print('Predicción del modelo: %.3f' % yhat)\n",
    "print('Valor obtenido de los datos: 17')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que según los resultados, el modelo se acerca bastante a lo esperado. El resultado no es igual pero cercano incluso hasta redodeado al entero más cercano se obtiene lo deseado. Gracias a este modelo se podrían obtener otros datos de abalón y a partir de estos conseguir la cantidad de anillos sin necesidad del experimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h)\n",
    "\n",
    "Realice 3 conclusiones sobre el ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los perceptrones multicapa permiten pasar los datos por varios niveles de regresión y así mejorar la precisión del modelo.\n",
    "- Los modelos de perceptrón multicapa se pueden adaptar a cualquier caso de conjutno de datos que tengan una dependencia entre sí, a pesar de que la dependencia no es trivial el modelo logra encontrar una rápidamente.\n",
    "- Gracias a la utilización de este modelo se obteiene una forma sencilla de obtener la edad de Abalón con base en datos datos mucho más sencillos de medir, esta facilidad acelera considerablemente el análisis de los próximos Abalones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 4. Ajuste de curvas con mínimos cuadrados.\n",
    "(requisito indispensable para tomar en cuenta el ejercicio, deben usar tensores de PyTorch).\n",
    "- a) (5 puntos) Calcule el $w_{opt}$ (el w óptimo) para los datos de los abulones. \n",
    "- b) (5 puntos) Implemente la función forward, la cual estima las salidas del modelo al hacer  $f(x) =X\\,\\vec{w}_{opt}$ donde la función f(x) se refiere a la función de activación, con X la matriz de características.\n",
    "- c) (5 puntos) Calcule la pérdida utilizando el error cuadrático medio.\n",
    "- d) (3 puntos) Realice al menos tres conclusiones sobre el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "Calcule el $w_{opt}$ (el w óptimo) para los datos de los abulones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor optimo:  tensor([[13.1107],\n",
      "        [-1.6755],\n",
      "        [33.6175],\n",
      "        [-5.4968],\n",
      "        [-9.9780],\n",
      "        [28.0774]])\n"
     ]
    }
   ],
   "source": [
    "# Se estima el W óptimo con mínimos cuadrados\n",
    " \n",
    "def estimateOptimumW(targetsAll, SamplesAll):\n",
    "    \"\"\" \n",
    "    Estimate the optimum W with least squares\n",
    "    param TargetsAll, target\n",
    "    param SamplesAll, NumSamples \n",
    "    return \n",
    "       wOpt, array with optimum weights\n",
    "    \"\"\"\n",
    "    # Calculate w = Apinverse*Targets\n",
    "    samplesAllPinv = torch.tensor(np.linalg.pinv(SamplesAll))\n",
    "    samplesAllPinv = samplesAllPinv.to(torch.float32)\n",
    "    targetsAll = targetsAll.to(torch.float32)\n",
    "    wOpt = samplesAllPinv.mm(targetsAll)\n",
    "    return wOpt\n",
    " \n",
    "# La informaición se convierte en arrays de numpy\n",
    "# para después pasarlos a tensores de Pytorch\n",
    "SamplesAll = torch.tensor(data.iloc[:,[0,1,2,3,4,5]].to_numpy())\n",
    "targetsAll = torch.tensor(data.iloc[:,[6]].to_numpy())\n",
    "\n",
    "# The targets are adjusted to be able to computet he multiplication. \n",
    "targetsAll=targetsAll.reshape([targetsAll.shape[0],1])\n",
    "\n",
    "# Se calcula el w óptimo\n",
    "wOpt = estimateOptimumW( targetsAll, SamplesAll)\n",
    "\n",
    "print(\"El valor optimo: \", wOpt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "Implemente la función forward, la cual estima las salidas del modelo al hacer  $f(x) =X\\,\\vec{w}_{opt}$ donde la función f(x) se refiere a la función de activación, con X la matriz de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15.3877],\n",
      "        [14.6100],\n",
      "        [16.5489],\n",
      "        [16.0999],\n",
      "        [14.1067],\n",
      "        [14.0875],\n",
      "        [13.8591],\n",
      "        [14.6315],\n",
      "        [12.8525],\n",
      "        [15.2496],\n",
      "        [13.2011],\n",
      "        [13.2040],\n",
      "        [14.2062],\n",
      "        [17.1331],\n",
      "        [12.6125],\n",
      "        [13.4489],\n",
      "        [14.8094],\n",
      "        [14.6450],\n",
      "        [12.2912],\n",
      "        [15.1556],\n",
      "        [13.0127],\n",
      "        [15.4105],\n",
      "        [17.8852],\n",
      "        [15.3657],\n",
      "        [16.8495],\n",
      "        [14.2335],\n",
      "        [15.9262],\n",
      "        [14.3349],\n",
      "        [16.4405],\n",
      "        [13.8627],\n",
      "        [15.4082],\n",
      "        [14.5187],\n",
      "        [15.6603],\n",
      "        [13.9832],\n",
      "        [16.1309],\n",
      "        [15.2123],\n",
      "        [12.7525],\n",
      "        [13.8879],\n",
      "        [13.4967],\n",
      "        [14.9629],\n",
      "        [11.2373],\n",
      "        [12.5461],\n",
      "        [14.7058],\n",
      "        [17.0341],\n",
      "        [15.0235],\n",
      "        [17.4306],\n",
      "        [14.6517],\n",
      "        [10.6152],\n",
      "        [15.9025],\n",
      "        [12.7010],\n",
      "        [15.1452],\n",
      "        [15.4699],\n",
      "        [12.8616],\n",
      "        [13.6418],\n",
      "        [10.0686],\n",
      "        [10.9678],\n",
      "        [11.3623],\n",
      "        [13.4009],\n",
      "        [11.7455],\n",
      "        [15.9022],\n",
      "        [15.2332],\n",
      "        [14.9012],\n",
      "        [16.1615],\n",
      "        [14.7238],\n",
      "        [13.3640],\n",
      "        [11.9981],\n",
      "        [14.4838],\n",
      "        [14.6647],\n",
      "        [15.2307],\n",
      "        [14.6851],\n",
      "        [14.7384],\n",
      "        [13.7520],\n",
      "        [15.9803],\n",
      "        [12.5993],\n",
      "        [14.5953],\n",
      "        [13.4942],\n",
      "        [16.8056],\n",
      "        [14.2625],\n",
      "        [15.8645],\n",
      "        [14.3512],\n",
      "        [14.6183],\n",
      "        [14.0089],\n",
      "        [15.8174],\n",
      "        [16.8480],\n",
      "        [13.1624],\n",
      "        [12.2915],\n",
      "        [17.6767],\n",
      "        [12.4924],\n",
      "        [13.4735],\n",
      "        [14.2450],\n",
      "        [16.7124],\n",
      "        [16.7438],\n",
      "        [15.1903],\n",
      "        [11.2606],\n",
      "        [12.7196],\n",
      "        [11.9585],\n",
      "        [11.9815],\n",
      "        [14.2589],\n",
      "        [13.9014],\n",
      "        [14.0626],\n",
      "        [14.7603],\n",
      "        [16.2013],\n",
      "        [14.1754],\n",
      "        [10.4452],\n",
      "        [11.9370],\n",
      "        [10.8045],\n",
      "        [14.1011],\n",
      "        [12.9505],\n",
      "        [14.9876],\n",
      "        [13.2584],\n",
      "        [11.0519],\n",
      "        [11.0300],\n",
      "        [13.4317],\n",
      "        [13.5680],\n",
      "        [14.6228],\n",
      "        [13.1576],\n",
      "        [11.7781],\n",
      "        [14.1372],\n",
      "        [13.6537],\n",
      "        [13.3599],\n",
      "        [13.9920],\n",
      "        [14.3085],\n",
      "        [13.9720],\n",
      "        [13.3810],\n",
      "        [14.9376],\n",
      "        [14.5005],\n",
      "        [15.1560],\n",
      "        [13.4496],\n",
      "        [13.6687],\n",
      "        [14.0414],\n",
      "        [12.5150],\n",
      "        [14.4537],\n",
      "        [14.2916],\n",
      "        [12.5996],\n",
      "        [12.2318],\n",
      "        [13.6814],\n",
      "        [12.5371],\n",
      "        [16.4129],\n",
      "        [13.9562],\n",
      "        [12.0324],\n",
      "        [14.3626],\n",
      "        [13.7645],\n",
      "        [14.7553],\n",
      "        [13.2679],\n",
      "        [12.6575],\n",
      "        [12.8024],\n",
      "        [12.9377],\n",
      "        [12.7480],\n",
      "        [17.5099],\n",
      "        [15.5534],\n",
      "        [13.4811],\n",
      "        [15.1917],\n",
      "        [14.2946],\n",
      "        [12.6829],\n",
      "        [12.7393],\n",
      "        [12.9284],\n",
      "        [11.7584],\n",
      "        [11.4188],\n",
      "        [11.7393],\n",
      "        [ 8.6382],\n",
      "        [12.1055],\n",
      "        [12.5368],\n",
      "        [14.1324],\n",
      "        [11.9970],\n",
      "        [14.7860],\n",
      "        [15.3272],\n",
      "        [13.6103],\n",
      "        [12.2757],\n",
      "        [13.5000],\n",
      "        [12.0557],\n",
      "        [15.7063],\n",
      "        [14.7009],\n",
      "        [12.6518],\n",
      "        [11.7848],\n",
      "        [12.8217],\n",
      "        [12.4925],\n",
      "        [11.0347],\n",
      "        [11.9538],\n",
      "        [10.5842],\n",
      "        [12.9267],\n",
      "        [13.9270],\n",
      "        [13.8063],\n",
      "        [10.5253],\n",
      "        [13.1583],\n",
      "        [11.4649],\n",
      "        [12.3610],\n",
      "        [13.8390],\n",
      "        [16.7154],\n",
      "        [12.5176],\n",
      "        [15.4786],\n",
      "        [12.2367],\n",
      "        [12.8855],\n",
      "        [13.7315],\n",
      "        [13.9411],\n",
      "        [13.4403],\n",
      "        [14.1860],\n",
      "        [15.0942],\n",
      "        [14.0540],\n",
      "        [15.0574],\n",
      "        [11.0376],\n",
      "        [12.6132],\n",
      "        [ 9.6867],\n",
      "        [10.8885],\n",
      "        [13.8220],\n",
      "        [11.5889],\n",
      "        [12.4777],\n",
      "        [13.4715],\n",
      "        [12.1027],\n",
      "        [15.0795],\n",
      "        [10.8646],\n",
      "        [14.1294],\n",
      "        [11.6369],\n",
      "        [12.1714],\n",
      "        [11.4223],\n",
      "        [ 9.0179],\n",
      "        [10.1371],\n",
      "        [11.7474],\n",
      "        [11.8147],\n",
      "        [13.7510],\n",
      "        [14.8181],\n",
      "        [16.7095],\n",
      "        [14.4952],\n",
      "        [11.4004],\n",
      "        [15.8504],\n",
      "        [11.8615],\n",
      "        [12.9371],\n",
      "        [14.7437],\n",
      "        [12.2091],\n",
      "        [11.9840],\n",
      "        [11.4017],\n",
      "        [10.8699],\n",
      "        [14.1034],\n",
      "        [15.7608],\n",
      "        [13.8431],\n",
      "        [12.0431],\n",
      "        [12.0130],\n",
      "        [10.9862],\n",
      "        [ 9.7718],\n",
      "        [13.9715],\n",
      "        [11.2069],\n",
      "        [12.2457],\n",
      "        [12.9208],\n",
      "        [11.5136],\n",
      "        [12.7592],\n",
      "        [11.0832],\n",
      "        [11.9896],\n",
      "        [11.9765],\n",
      "        [12.0775],\n",
      "        [10.6686],\n",
      "        [11.0847],\n",
      "        [12.9046],\n",
      "        [10.7050],\n",
      "        [10.1366],\n",
      "        [11.5289],\n",
      "        [12.1625],\n",
      "        [14.0736],\n",
      "        [12.2616],\n",
      "        [12.2310],\n",
      "        [12.9707],\n",
      "        [14.7920],\n",
      "        [12.4347],\n",
      "        [13.7881],\n",
      "        [14.7217],\n",
      "        [13.1956],\n",
      "        [15.6049],\n",
      "        [14.8056],\n",
      "        [17.0961],\n",
      "        [16.7292],\n",
      "        [12.8914],\n",
      "        [11.6851],\n",
      "        [12.5687],\n",
      "        [12.7329],\n",
      "        [11.0863],\n",
      "        [10.6662],\n",
      "        [12.7549],\n",
      "        [12.4613],\n",
      "        [13.8011],\n",
      "        [12.3224],\n",
      "        [12.0900],\n",
      "        [12.4229],\n",
      "        [15.2421],\n",
      "        [16.6913],\n",
      "        [11.3739],\n",
      "        [14.8403],\n",
      "        [15.4764],\n",
      "        [16.8539],\n",
      "        [12.6416],\n",
      "        [13.0424],\n",
      "        [11.6226],\n",
      "        [14.9254],\n",
      "        [13.1484],\n",
      "        [12.2709],\n",
      "        [12.7664],\n",
      "        [13.3499],\n",
      "        [13.2103],\n",
      "        [12.5738],\n",
      "        [11.9150],\n",
      "        [14.1494],\n",
      "        [12.9650],\n",
      "        [12.7356],\n",
      "        [12.0318],\n",
      "        [15.8322],\n",
      "        [11.1187],\n",
      "        [14.0775],\n",
      "        [10.3773],\n",
      "        [11.9819],\n",
      "        [11.9718],\n",
      "        [13.3092],\n",
      "        [12.3560],\n",
      "        [10.3895],\n",
      "        [11.5329],\n",
      "        [12.2838],\n",
      "        [11.9630],\n",
      "        [11.7365],\n",
      "        [13.6475],\n",
      "        [14.0694],\n",
      "        [11.6632],\n",
      "        [12.5076],\n",
      "        [11.8189],\n",
      "        [11.9848],\n",
      "        [12.7022],\n",
      "        [10.5763],\n",
      "        [12.8566],\n",
      "        [13.5991],\n",
      "        [11.2880],\n",
      "        [14.8554],\n",
      "        [10.8324],\n",
      "        [10.7616],\n",
      "        [12.7311],\n",
      "        [12.8854],\n",
      "        [13.8997],\n",
      "        [11.9693],\n",
      "        [10.7468],\n",
      "        [12.5806],\n",
      "        [10.5555],\n",
      "        [11.2832],\n",
      "        [11.7580],\n",
      "        [13.7975],\n",
      "        [15.0007],\n",
      "        [12.0474],\n",
      "        [11.2830],\n",
      "        [12.3941],\n",
      "        [11.8276],\n",
      "        [ 9.4666],\n",
      "        [10.8327],\n",
      "        [11.4973],\n",
      "        [12.9803],\n",
      "        [14.7543],\n",
      "        [13.2436],\n",
      "        [12.1566],\n",
      "        [11.9706],\n",
      "        [13.5143],\n",
      "        [12.7201],\n",
      "        [11.1537],\n",
      "        [14.4135],\n",
      "        [14.3891],\n",
      "        [10.7405],\n",
      "        [13.7072],\n",
      "        [13.5906],\n",
      "        [11.4855],\n",
      "        [12.3088],\n",
      "        [10.1670],\n",
      "        [11.5210],\n",
      "        [10.0657],\n",
      "        [11.5464],\n",
      "        [14.8759],\n",
      "        [10.4883],\n",
      "        [14.5145],\n",
      "        [11.0647],\n",
      "        [10.3405],\n",
      "        [12.3749],\n",
      "        [13.6269],\n",
      "        [12.6306],\n",
      "        [11.3692],\n",
      "        [11.7527],\n",
      "        [10.8668],\n",
      "        [14.1061],\n",
      "        [12.5300],\n",
      "        [11.5335],\n",
      "        [12.4003],\n",
      "        [13.2220],\n",
      "        [11.4924],\n",
      "        [12.1509],\n",
      "        [11.8598],\n",
      "        [14.7927],\n",
      "        [11.8668],\n",
      "        [10.2278],\n",
      "        [ 9.9665],\n",
      "        [10.5121],\n",
      "        [12.0436],\n",
      "        [11.7060],\n",
      "        [11.1881],\n",
      "        [14.3494],\n",
      "        [13.3978],\n",
      "        [11.3155],\n",
      "        [11.9166],\n",
      "        [11.9915],\n",
      "        [10.9655],\n",
      "        [13.1614],\n",
      "        [11.3371],\n",
      "        [10.9828],\n",
      "        [11.9239],\n",
      "        [11.8285],\n",
      "        [11.2904],\n",
      "        [10.1019],\n",
      "        [10.0946],\n",
      "        [11.1213],\n",
      "        [10.8756],\n",
      "        [12.9586],\n",
      "        [12.1124],\n",
      "        [10.0756],\n",
      "        [10.9289],\n",
      "        [10.3968],\n",
      "        [11.2501],\n",
      "        [12.5184],\n",
      "        [11.8188],\n",
      "        [14.4704],\n",
      "        [ 9.2225],\n",
      "        [11.2971],\n",
      "        [10.4008],\n",
      "        [10.9691],\n",
      "        [11.8260],\n",
      "        [ 9.7157],\n",
      "        [13.9314],\n",
      "        [10.3729],\n",
      "        [11.9744],\n",
      "        [11.0642],\n",
      "        [12.0825],\n",
      "        [13.8682],\n",
      "        [11.5334],\n",
      "        [11.0500],\n",
      "        [10.1244],\n",
      "        [ 9.4807],\n",
      "        [11.1566],\n",
      "        [11.1886],\n",
      "        [12.0664],\n",
      "        [ 9.2792],\n",
      "        [10.6159],\n",
      "        [10.3779],\n",
      "        [10.9186],\n",
      "        [ 9.7600],\n",
      "        [10.9439],\n",
      "        [11.8907],\n",
      "        [11.3473],\n",
      "        [10.6746],\n",
      "        [ 9.7092],\n",
      "        [10.1772],\n",
      "        [10.2273],\n",
      "        [ 9.9313],\n",
      "        [ 9.7917],\n",
      "        [11.1391],\n",
      "        [10.1768],\n",
      "        [10.5647],\n",
      "        [11.8861],\n",
      "        [12.2030],\n",
      "        [12.3237],\n",
      "        [11.4936],\n",
      "        [11.1446],\n",
      "        [10.9558],\n",
      "        [12.0123],\n",
      "        [10.3765],\n",
      "        [12.1050],\n",
      "        [10.9012],\n",
      "        [10.2379],\n",
      "        [10.3178],\n",
      "        [ 9.6322],\n",
      "        [10.9167],\n",
      "        [12.0143],\n",
      "        [11.3814],\n",
      "        [11.2832],\n",
      "        [12.1913],\n",
      "        [12.8538],\n",
      "        [12.2928],\n",
      "        [11.6038],\n",
      "        [11.2996],\n",
      "        [11.8651],\n",
      "        [11.6495],\n",
      "        [12.0822],\n",
      "        [13.2648],\n",
      "        [10.9274],\n",
      "        [10.3403],\n",
      "        [10.1449],\n",
      "        [10.3833],\n",
      "        [11.8580],\n",
      "        [10.3832],\n",
      "        [11.3616],\n",
      "        [10.1917],\n",
      "        [ 9.6777],\n",
      "        [ 9.6289],\n",
      "        [ 9.9397],\n",
      "        [10.3175],\n",
      "        [10.0636],\n",
      "        [11.9214],\n",
      "        [10.3795],\n",
      "        [11.4890],\n",
      "        [12.4702],\n",
      "        [10.5736],\n",
      "        [ 9.7896],\n",
      "        [ 9.6163],\n",
      "        [11.7362],\n",
      "        [12.1268],\n",
      "        [10.7849],\n",
      "        [11.7551],\n",
      "        [10.3906],\n",
      "        [ 9.4648],\n",
      "        [11.3204],\n",
      "        [ 9.8789],\n",
      "        [11.5175],\n",
      "        [11.0558],\n",
      "        [11.1997],\n",
      "        [11.8403],\n",
      "        [11.1753],\n",
      "        [ 9.8417],\n",
      "        [11.0426],\n",
      "        [10.8697],\n",
      "        [10.6597],\n",
      "        [ 9.9358],\n",
      "        [10.6838],\n",
      "        [12.7688],\n",
      "        [11.9933],\n",
      "        [10.8310],\n",
      "        [10.1528],\n",
      "        [ 8.6213],\n",
      "        [10.3405],\n",
      "        [10.2825],\n",
      "        [10.9071],\n",
      "        [11.0961],\n",
      "        [10.8000],\n",
      "        [ 9.1107],\n",
      "        [11.1189],\n",
      "        [10.6315],\n",
      "        [ 9.8803],\n",
      "        [ 9.6437],\n",
      "        [ 9.9269],\n",
      "        [ 9.7444],\n",
      "        [10.6807],\n",
      "        [ 9.4685],\n",
      "        [10.6056],\n",
      "        [10.4908],\n",
      "        [ 9.5228],\n",
      "        [10.0250],\n",
      "        [ 8.8797],\n",
      "        [10.5195],\n",
      "        [ 9.8061],\n",
      "        [10.3129],\n",
      "        [10.1255],\n",
      "        [10.7357],\n",
      "        [10.4484],\n",
      "        [10.4567],\n",
      "        [ 9.3047],\n",
      "        [10.4922],\n",
      "        [12.0218],\n",
      "        [10.4612],\n",
      "        [ 9.8574],\n",
      "        [ 8.8754],\n",
      "        [ 8.0567],\n",
      "        [10.1272],\n",
      "        [10.3908],\n",
      "        [ 9.7833],\n",
      "        [10.3460],\n",
      "        [ 9.6671],\n",
      "        [10.8287],\n",
      "        [11.4272],\n",
      "        [13.3347],\n",
      "        [10.4088],\n",
      "        [10.7667],\n",
      "        [10.3364],\n",
      "        [10.9335],\n",
      "        [10.2893],\n",
      "        [10.3573],\n",
      "        [10.8706],\n",
      "        [10.3481],\n",
      "        [11.2135],\n",
      "        [10.6272],\n",
      "        [10.4678],\n",
      "        [11.5848],\n",
      "        [12.0703],\n",
      "        [10.1891],\n",
      "        [ 8.6547],\n",
      "        [ 9.5668],\n",
      "        [ 8.6682],\n",
      "        [10.4999],\n",
      "        [ 9.6568],\n",
      "        [ 9.9304],\n",
      "        [10.8883],\n",
      "        [ 9.0592],\n",
      "        [ 8.6050],\n",
      "        [10.8861],\n",
      "        [ 9.0337],\n",
      "        [ 9.4445],\n",
      "        [ 9.2558],\n",
      "        [ 9.0796],\n",
      "        [10.0195],\n",
      "        [10.4599],\n",
      "        [ 9.9643],\n",
      "        [ 9.2228],\n",
      "        [11.6509],\n",
      "        [10.1321],\n",
      "        [ 9.8863],\n",
      "        [ 9.3318],\n",
      "        [10.5479],\n",
      "        [10.0978],\n",
      "        [11.3056],\n",
      "        [ 8.5427],\n",
      "        [ 9.1236],\n",
      "        [ 8.9260],\n",
      "        [10.1078],\n",
      "        [ 8.7163],\n",
      "        [ 9.7636],\n",
      "        [ 9.8109],\n",
      "        [ 9.1873],\n",
      "        [ 9.5019],\n",
      "        [ 9.2226],\n",
      "        [ 9.1117],\n",
      "        [ 8.8716],\n",
      "        [ 8.8532],\n",
      "        [ 9.2576],\n",
      "        [ 9.9090],\n",
      "        [ 9.8381],\n",
      "        [10.5286],\n",
      "        [ 9.7450],\n",
      "        [ 9.0116],\n",
      "        [ 9.7070],\n",
      "        [ 9.9067],\n",
      "        [ 9.8511],\n",
      "        [10.9560],\n",
      "        [10.9512],\n",
      "        [ 9.8133],\n",
      "        [10.9033],\n",
      "        [10.8922],\n",
      "        [11.8517],\n",
      "        [ 9.7986],\n",
      "        [ 9.2199],\n",
      "        [ 9.6189],\n",
      "        [ 9.2119],\n",
      "        [ 8.7699],\n",
      "        [ 8.2259],\n",
      "        [ 9.6872],\n",
      "        [ 9.7992],\n",
      "        [10.7209],\n",
      "        [ 9.7184],\n",
      "        [ 9.5181],\n",
      "        [ 8.7497],\n",
      "        [ 8.9671],\n",
      "        [ 8.1885],\n",
      "        [ 8.0748],\n",
      "        [ 8.8397],\n",
      "        [ 9.7246],\n",
      "        [ 9.7375],\n",
      "        [ 8.8637],\n",
      "        [10.1991],\n",
      "        [10.1599],\n",
      "        [10.6658],\n",
      "        [10.0111],\n",
      "        [ 9.4788],\n",
      "        [ 9.0069],\n",
      "        [ 9.5140],\n",
      "        [ 9.2239],\n",
      "        [12.0908],\n",
      "        [ 9.9626],\n",
      "        [ 9.2072],\n",
      "        [ 9.7610],\n",
      "        [ 9.5445],\n",
      "        [10.0194],\n",
      "        [ 8.6055],\n",
      "        [ 9.2453],\n",
      "        [ 8.3433],\n",
      "        [ 9.2666],\n",
      "        [ 9.8429],\n",
      "        [ 8.9814],\n",
      "        [ 8.7635],\n",
      "        [ 9.3040],\n",
      "        [ 9.5468],\n",
      "        [ 9.2786],\n",
      "        [ 9.4997],\n",
      "        [ 9.4275],\n",
      "        [ 9.9498],\n",
      "        [ 8.8605],\n",
      "        [ 9.4582],\n",
      "        [ 8.5762],\n",
      "        [ 7.5818],\n",
      "        [ 8.9270],\n",
      "        [ 8.1502],\n",
      "        [10.3594],\n",
      "        [ 9.6497],\n",
      "        [ 8.6950],\n",
      "        [10.3907],\n",
      "        [ 9.3355],\n",
      "        [ 9.1542],\n",
      "        [10.3872],\n",
      "        [ 8.2113],\n",
      "        [ 8.1142],\n",
      "        [ 8.5635],\n",
      "        [ 9.4438],\n",
      "        [ 9.8602],\n",
      "        [ 8.6593],\n",
      "        [ 8.1861],\n",
      "        [ 8.9235],\n",
      "        [ 9.7360],\n",
      "        [ 8.8454],\n",
      "        [ 9.2146],\n",
      "        [10.7338],\n",
      "        [ 9.7077],\n",
      "        [10.3411],\n",
      "        [ 8.8568],\n",
      "        [ 8.5564],\n",
      "        [ 7.6563],\n",
      "        [ 8.9636],\n",
      "        [ 8.3408],\n",
      "        [ 8.6707],\n",
      "        [ 8.3495],\n",
      "        [ 7.8517],\n",
      "        [ 8.0112],\n",
      "        [ 7.9948],\n",
      "        [ 8.2115],\n",
      "        [ 8.0173],\n",
      "        [ 9.1656],\n",
      "        [ 9.4504],\n",
      "        [ 7.9916],\n",
      "        [ 9.2119],\n",
      "        [ 8.8465],\n",
      "        [ 8.4453],\n",
      "        [ 8.2906],\n",
      "        [ 9.4089],\n",
      "        [ 9.0867],\n",
      "        [ 8.6201],\n",
      "        [ 9.0528],\n",
      "        [ 8.0625],\n",
      "        [ 8.4452],\n",
      "        [ 8.6161],\n",
      "        [ 8.3515],\n",
      "        [ 8.6959],\n",
      "        [ 9.3224],\n",
      "        [ 8.3535],\n",
      "        [ 8.8590],\n",
      "        [ 8.8858],\n",
      "        [ 7.6998],\n",
      "        [ 8.3457],\n",
      "        [ 7.8303],\n",
      "        [ 7.6798],\n",
      "        [ 8.3855],\n",
      "        [ 8.3233],\n",
      "        [ 8.2173],\n",
      "        [ 8.3566],\n",
      "        [ 8.2764],\n",
      "        [ 8.5404],\n",
      "        [ 9.4469],\n",
      "        [ 7.8215],\n",
      "        [ 8.2138],\n",
      "        [ 7.5096],\n",
      "        [ 8.5412],\n",
      "        [ 7.8986],\n",
      "        [ 8.4911],\n",
      "        [ 8.1205],\n",
      "        [ 8.2393],\n",
      "        [ 8.3696],\n",
      "        [ 8.7077],\n",
      "        [ 8.0741],\n",
      "        [ 7.5947],\n",
      "        [ 8.2654],\n",
      "        [ 8.5677],\n",
      "        [ 8.6361],\n",
      "        [ 8.2771],\n",
      "        [ 7.2367],\n",
      "        [ 7.2085],\n",
      "        [ 7.8434],\n",
      "        [ 8.1475],\n",
      "        [ 7.8424],\n",
      "        [ 8.3940],\n",
      "        [ 8.0827],\n",
      "        [ 8.4546],\n",
      "        [ 8.5157],\n",
      "        [ 7.9919],\n",
      "        [ 7.9187],\n",
      "        [ 9.1519],\n",
      "        [ 7.4925],\n",
      "        [ 7.5443],\n",
      "        [ 7.9548],\n",
      "        [ 7.5452],\n",
      "        [ 7.8130],\n",
      "        [ 8.5389],\n",
      "        [ 9.1635],\n",
      "        [ 7.8438],\n",
      "        [ 7.4370],\n",
      "        [ 8.0653],\n",
      "        [ 7.5823],\n",
      "        [ 7.9818],\n",
      "        [ 7.9501],\n",
      "        [ 8.5692],\n",
      "        [ 8.1646],\n",
      "        [ 8.3529],\n",
      "        [ 8.5363],\n",
      "        [ 8.4839],\n",
      "        [ 7.4170],\n",
      "        [ 8.0122],\n",
      "        [ 9.3290],\n",
      "        [ 7.2822],\n",
      "        [ 7.0438],\n",
      "        [ 8.2806],\n",
      "        [ 7.5165],\n",
      "        [ 7.0627],\n",
      "        [ 9.4547],\n",
      "        [ 7.5568],\n",
      "        [ 7.6722],\n",
      "        [ 7.0894],\n",
      "        [ 7.3855],\n",
      "        [ 7.4123],\n",
      "        [ 7.5375],\n",
      "        [ 7.3946],\n",
      "        [ 8.1255],\n",
      "        [ 6.6632],\n",
      "        [ 7.6848],\n",
      "        [ 7.1830],\n",
      "        [ 7.1994],\n",
      "        [ 6.5709],\n",
      "        [ 7.4940],\n",
      "        [ 8.0168],\n",
      "        [ 7.5911],\n",
      "        [ 6.9696],\n",
      "        [ 7.2271],\n",
      "        [ 6.9485],\n",
      "        [ 7.6892],\n",
      "        [ 7.6481],\n",
      "        [ 7.3827],\n",
      "        [ 6.6025],\n",
      "        [ 7.2285],\n",
      "        [ 6.3884],\n",
      "        [ 6.2892],\n",
      "        [ 7.2744],\n",
      "        [ 7.8047],\n",
      "        [ 7.6445],\n",
      "        [ 7.3775],\n",
      "        [ 6.8720],\n",
      "        [ 6.5966],\n",
      "        [ 6.6120],\n",
      "        [ 7.1426],\n",
      "        [ 7.8302],\n",
      "        [ 7.5777],\n",
      "        [ 7.2979],\n",
      "        [ 6.5765],\n",
      "        [ 6.6078],\n",
      "        [ 6.3278],\n",
      "        [ 7.1799],\n",
      "        [ 7.0174],\n",
      "        [ 6.8586],\n",
      "        [ 6.7901],\n",
      "        [ 6.9787],\n",
      "        [ 6.6094],\n",
      "        [ 6.3121],\n",
      "        [ 6.6827],\n",
      "        [ 7.5762],\n",
      "        [ 6.4481],\n",
      "        [ 6.6232],\n",
      "        [ 6.8820],\n",
      "        [ 5.7165],\n",
      "        [ 6.7180],\n",
      "        [ 6.7877],\n",
      "        [ 7.4837],\n",
      "        [ 6.8005],\n",
      "        [ 6.2603],\n",
      "        [ 6.1250],\n",
      "        [ 6.0532],\n",
      "        [ 6.6500],\n",
      "        [ 6.3980],\n",
      "        [ 6.8018],\n",
      "        [ 6.4488],\n",
      "        [ 6.1364],\n",
      "        [ 5.9379],\n",
      "        [ 6.3951],\n",
      "        [ 6.1525],\n",
      "        [ 6.4517],\n",
      "        [ 6.6549],\n",
      "        [ 6.3023],\n",
      "        [ 6.4452],\n",
      "        [ 6.1484],\n",
      "        [ 6.5629],\n",
      "        [ 5.8606],\n",
      "        [ 5.9714],\n",
      "        [ 6.1663],\n",
      "        [ 6.0691],\n",
      "        [ 6.1089],\n",
      "        [ 6.1392],\n",
      "        [ 6.1501],\n",
      "        [ 6.1164],\n",
      "        [ 6.6069],\n",
      "        [ 5.2188],\n",
      "        [ 5.6658],\n",
      "        [ 6.0274],\n",
      "        [ 5.8250],\n",
      "        [ 5.8742],\n",
      "        [ 5.6796],\n",
      "        [ 5.8115],\n",
      "        [ 5.5243],\n",
      "        [ 5.4368],\n",
      "        [ 5.7466],\n",
      "        [ 6.0756],\n",
      "        [ 6.0007],\n",
      "        [ 5.3701],\n",
      "        [ 5.2319],\n",
      "        [ 5.4778],\n",
      "        [ 5.5640],\n",
      "        [ 5.4681],\n",
      "        [ 5.3404],\n",
      "        [ 5.3548],\n",
      "        [ 5.5681],\n",
      "        [ 5.0028],\n",
      "        [ 4.7931],\n",
      "        [ 4.7814],\n",
      "        [ 5.1290],\n",
      "        [ 5.0004],\n",
      "        [ 4.8658],\n",
      "        [ 4.3084],\n",
      "        [ 5.3191],\n",
      "        [ 4.7647],\n",
      "        [ 5.0156],\n",
      "        [ 4.1354],\n",
      "        [ 4.1569],\n",
      "        [ 4.8568],\n",
      "        [ 4.8048],\n",
      "        [ 4.6492],\n",
      "        [ 5.0096],\n",
      "        [ 4.2105],\n",
      "        [ 4.5991],\n",
      "        [ 4.5441],\n",
      "        [ 4.2484],\n",
      "        [ 4.1671],\n",
      "        [ 4.2417],\n",
      "        [ 4.0875],\n",
      "        [ 4.1405],\n",
      "        [ 4.4069],\n",
      "        [ 4.2451],\n",
      "        [ 4.0757],\n",
      "        [ 3.7402],\n",
      "        [ 3.3652],\n",
      "        [ 4.0385],\n",
      "        [ 3.8263],\n",
      "        [ 3.3782],\n",
      "        [ 5.2407],\n",
      "        [ 2.9431],\n",
      "        [ 2.6623],\n",
      "        [ 3.0533],\n",
      "        [ 2.7404],\n",
      "        [ 3.2178],\n",
      "        [ 3.5464],\n",
      "        [ 2.6575],\n",
      "        [ 2.5563],\n",
      "        [ 2.3202],\n",
      "        [ 1.2535]])\n"
     ]
    }
   ],
   "source": [
    "def forward(SamplesAll, wOpt):\n",
    "    \"\"\"\n",
    "    Get model output\n",
    "    param: SamplesAll, a matrix with dimensions NumSamples x NumDimensions \n",
    "    return: Estimates the model outputs activation function \n",
    "    \"\"\"\n",
    "    SamplesAll = SamplesAll.to(torch.float32)\n",
    "    wOpt = wOpt.to(torch.float32)\n",
    "    \n",
    "    EstimatedTargets = SamplesAll.mm(wOpt)\n",
    "    \n",
    "    return EstimatedTargets\n",
    "\n",
    "# Estimated target for sample data    \n",
    "estimatedTargetsAll = forward(SamplesAll, wOpt)\n",
    "\n",
    "print(estimatedTargetsAll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Calcule la pérdida utilizando el error cuadrático medio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 82.901, RMSE: 9.105\n"
     ]
    }
   ],
   "source": [
    "# 4) Se evalúa el error\n",
    "def evaluateError(TargetsAll, EstimatedTargetsAll):\n",
    "    \"\"\"\n",
    "    Evaluate model error using the euclidian distance.\n",
    "    param TargetsAll, real targets\n",
    "    param EstimatedTargets\n",
    "    \"\"\"\n",
    "    error = torch.norm(TargetsAll - EstimatedTargetsAll, 2)\n",
    "    return error\n",
    "  \n",
    "  \n",
    "# Error for sample data  \n",
    "mse = evaluateError(targetsAll, estimatedTargetsAll)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "Realice al menos tres conclusiones sobre el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El error cuadrático medio de una aproximación por medio de perceptrones es por lo general menos que la aproximación por medio de perceptrones multicapa ya que se pasa por menos análisis.\n",
    "- A pesar de tener un error cuadrático medio mayor note que la diferencia con respecto a las aproximaciones del ejercicio anterior no son tan lejanas, por lo que en la ausencia de recursos se podría usar esata aproximación sin problemas.\n",
    "- El ajuste a la ecuación de mejor ajuste para los abalones permite obtener de forma lineal una forma de predecir el comportamiento de la cantidad de anillos que poseeera el abalon y por lo tanto facilita la investigación al hacerla mucho más rápido con predicciones confiables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 5. Implemente un perceptrón de una capa \n",
    "\n",
    "(requisito indispensable para tomar en cuenta el ejercicio, deben usar tensores de PyTorch).\n",
    "\n",
    "- a)(10 puntos) Implemente el algoritmo del perceptrón de una capa rescindiendo al máximo de estructuras de tipo for, usando en su lugar operaciones matriciales. Debe implementarlo sin utilizar ninguna biblioteca, es decir en PyTorch no se puede usar ninguna clase o funcionalidad desarrollada por PyTorch o alguna otra biblioteca.\n",
    "- b)(5 puntos) Utilice el perceptrón desarrollado en a) para realizar regresión con los datos de los abulones.\n",
    "- c) (3 puntos) Realice al menos tres conclusiones sobre el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "Implemente el algoritmo del perceptrón de una capa rescindiendo al máximo de estructuras de tipo for, usando en su lugar operaciones matriciales.\n",
    "\n",
    "### b)\n",
    "\n",
    "Utilice el perceptrón desarrollado en a) para realizar regresión con los datos de los abulones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18.4987],\n",
      "        [21.6167],\n",
      "        [19.9886],\n",
      "        [20.9833],\n",
      "        [21.5479],\n",
      "        [17.9752],\n",
      "        [20.7373],\n",
      "        [20.0601],\n",
      "        [21.5280],\n",
      "        [16.3790],\n",
      "        [20.9782],\n",
      "        [19.6770],\n",
      "        [18.6613],\n",
      "        [19.0843],\n",
      "        [21.3992],\n",
      "        [19.0787],\n",
      "        [18.5971],\n",
      "        [18.1823],\n",
      "        [18.0720],\n",
      "        [20.5646],\n",
      "        [18.9642],\n",
      "        [17.9968],\n",
      "        [18.3175],\n",
      "        [21.2679],\n",
      "        [18.1676],\n",
      "        [16.8948],\n",
      "        [17.9552],\n",
      "        [19.0630],\n",
      "        [18.0787],\n",
      "        [19.2560],\n",
      "        [18.3127],\n",
      "        [17.5240],\n",
      "        [17.5016],\n",
      "        [18.2788],\n",
      "        [18.3463],\n",
      "        [18.6576],\n",
      "        [19.5764],\n",
      "        [15.9948],\n",
      "        [18.7723],\n",
      "        [19.9341],\n",
      "        [21.0300],\n",
      "        [17.4987],\n",
      "        [19.1356],\n",
      "        [17.7975],\n",
      "        [17.0930],\n",
      "        [17.5989],\n",
      "        [20.1104],\n",
      "        [18.3617],\n",
      "        [18.3986],\n",
      "        [17.9854],\n",
      "        [19.0710],\n",
      "        [17.1742],\n",
      "        [17.0978],\n",
      "        [17.8662],\n",
      "        [17.7200],\n",
      "        [16.5259],\n",
      "        [15.5943],\n",
      "        [18.1219],\n",
      "        [16.5819],\n",
      "        [19.5046],\n",
      "        [16.7514],\n",
      "        [17.2326],\n",
      "        [17.7437],\n",
      "        [15.4684],\n",
      "        [16.0458],\n",
      "        [16.5450],\n",
      "        [17.3998],\n",
      "        [18.3751],\n",
      "        [15.0744],\n",
      "        [17.1898],\n",
      "        [16.0382],\n",
      "        [17.2836],\n",
      "        [18.6550],\n",
      "        [16.9705],\n",
      "        [17.3841],\n",
      "        [14.5999],\n",
      "        [16.5644],\n",
      "        [16.6288],\n",
      "        [16.1346],\n",
      "        [17.8689],\n",
      "        [14.9778],\n",
      "        [16.8391],\n",
      "        [16.7968],\n",
      "        [17.0934],\n",
      "        [17.4915],\n",
      "        [17.3403],\n",
      "        [18.1297],\n",
      "        [16.8608],\n",
      "        [16.6356],\n",
      "        [15.0729],\n",
      "        [17.0614],\n",
      "        [15.9473],\n",
      "        [16.5479],\n",
      "        [18.9209],\n",
      "        [17.5149],\n",
      "        [16.7781],\n",
      "        [16.0868],\n",
      "        [15.6068],\n",
      "        [15.7836],\n",
      "        [15.4095],\n",
      "        [14.7573],\n",
      "        [16.3307],\n",
      "        [16.0918],\n",
      "        [17.4952],\n",
      "        [17.1497],\n",
      "        [17.1215],\n",
      "        [17.5929],\n",
      "        [16.7684],\n",
      "        [15.3090],\n",
      "        [15.3852],\n",
      "        [15.7365],\n",
      "        [15.8394],\n",
      "        [16.1672],\n",
      "        [15.5126],\n",
      "        [14.5373],\n",
      "        [15.5969],\n",
      "        [15.8980],\n",
      "        [14.3754],\n",
      "        [15.7009],\n",
      "        [15.8792],\n",
      "        [15.9323],\n",
      "        [15.3530],\n",
      "        [15.2692],\n",
      "        [16.6713],\n",
      "        [16.2499],\n",
      "        [16.6748],\n",
      "        [15.8191],\n",
      "        [14.7039],\n",
      "        [14.1593],\n",
      "        [15.2729],\n",
      "        [15.8217],\n",
      "        [18.0602],\n",
      "        [17.3324],\n",
      "        [15.6742],\n",
      "        [14.4909],\n",
      "        [13.9831],\n",
      "        [16.5566],\n",
      "        [15.4269],\n",
      "        [15.6311],\n",
      "        [14.9895],\n",
      "        [14.7004],\n",
      "        [14.7343],\n",
      "        [16.1297],\n",
      "        [15.4568],\n",
      "        [13.9356],\n",
      "        [17.2220],\n",
      "        [15.5152],\n",
      "        [16.8875],\n",
      "        [17.3144],\n",
      "        [16.6229],\n",
      "        [14.0649],\n",
      "        [15.8819],\n",
      "        [16.9530],\n",
      "        [14.7517],\n",
      "        [17.2625],\n",
      "        [15.3330],\n",
      "        [16.2577],\n",
      "        [15.2433],\n",
      "        [15.1788],\n",
      "        [14.1904],\n",
      "        [13.8943],\n",
      "        [15.1162],\n",
      "        [15.9525],\n",
      "        [15.2709],\n",
      "        [16.5527],\n",
      "        [17.0385],\n",
      "        [14.7807],\n",
      "        [13.6307],\n",
      "        [15.5984],\n",
      "        [14.5073],\n",
      "        [15.4055],\n",
      "        [13.6039],\n",
      "        [14.1823],\n",
      "        [15.1178],\n",
      "        [15.1774],\n",
      "        [14.9198],\n",
      "        [15.3077],\n",
      "        [15.8013],\n",
      "        [14.5851],\n",
      "        [14.3859],\n",
      "        [14.0603],\n",
      "        [13.2794],\n",
      "        [14.9391],\n",
      "        [14.2778],\n",
      "        [13.8521],\n",
      "        [14.3311],\n",
      "        [13.6129],\n",
      "        [15.4295],\n",
      "        [14.3277],\n",
      "        [14.2683],\n",
      "        [15.5093],\n",
      "        [14.7117],\n",
      "        [14.7683],\n",
      "        [14.6117],\n",
      "        [13.8480],\n",
      "        [14.5179],\n",
      "        [14.0134],\n",
      "        [15.2158],\n",
      "        [13.1348],\n",
      "        [13.8948],\n",
      "        [13.2703],\n",
      "        [14.6870],\n",
      "        [14.1625],\n",
      "        [14.8279],\n",
      "        [14.0923],\n",
      "        [13.5565],\n",
      "        [13.0545],\n",
      "        [13.3914],\n",
      "        [17.2231],\n",
      "        [15.4865],\n",
      "        [15.1857],\n",
      "        [14.9635],\n",
      "        [14.2724],\n",
      "        [14.9676],\n",
      "        [15.0023],\n",
      "        [12.9047],\n",
      "        [13.2546],\n",
      "        [13.9970],\n",
      "        [14.4426],\n",
      "        [14.8701],\n",
      "        [15.5681],\n",
      "        [15.2560],\n",
      "        [15.2101],\n",
      "        [14.8108],\n",
      "        [12.8940],\n",
      "        [13.8436],\n",
      "        [13.4856],\n",
      "        [13.7597],\n",
      "        [13.7138],\n",
      "        [15.3414],\n",
      "        [14.6926],\n",
      "        [16.1892],\n",
      "        [16.8171],\n",
      "        [14.1248],\n",
      "        [12.9242],\n",
      "        [13.6140],\n",
      "        [13.6500],\n",
      "        [12.6490],\n",
      "        [13.8389],\n",
      "        [12.8348],\n",
      "        [14.6128],\n",
      "        [12.1608],\n",
      "        [14.1630],\n",
      "        [13.8241],\n",
      "        [14.7728],\n",
      "        [14.0676],\n",
      "        [13.8519],\n",
      "        [13.8761],\n",
      "        [11.9604],\n",
      "        [15.5156],\n",
      "        [15.4883],\n",
      "        [14.9058],\n",
      "        [14.2100],\n",
      "        [13.3226],\n",
      "        [13.7042],\n",
      "        [13.7410],\n",
      "        [12.4884],\n",
      "        [12.9599],\n",
      "        [13.7393],\n",
      "        [13.7830],\n",
      "        [13.7241],\n",
      "        [16.9538],\n",
      "        [14.6568],\n",
      "        [14.6107],\n",
      "        [14.5770],\n",
      "        [13.6156],\n",
      "        [14.2450],\n",
      "        [14.0982],\n",
      "        [12.7517],\n",
      "        [13.8321],\n",
      "        [12.4743],\n",
      "        [13.7420],\n",
      "        [14.5182],\n",
      "        [11.3349],\n",
      "        [13.5385],\n",
      "        [13.2179],\n",
      "        [13.2596],\n",
      "        [12.7156],\n",
      "        [14.6667],\n",
      "        [13.4078],\n",
      "        [14.6939],\n",
      "        [15.0917],\n",
      "        [13.6805],\n",
      "        [13.6536],\n",
      "        [12.2921],\n",
      "        [13.5588],\n",
      "        [13.3478],\n",
      "        [12.6574],\n",
      "        [13.2533],\n",
      "        [14.5466],\n",
      "        [13.8168],\n",
      "        [13.9101],\n",
      "        [12.2719],\n",
      "        [12.4469],\n",
      "        [12.2363],\n",
      "        [12.7419],\n",
      "        [13.7729],\n",
      "        [13.5779],\n",
      "        [13.2210],\n",
      "        [14.0740],\n",
      "        [14.5839],\n",
      "        [15.2979],\n",
      "        [13.3010],\n",
      "        [13.2534],\n",
      "        [12.0631],\n",
      "        [12.8621],\n",
      "        [12.8225],\n",
      "        [12.1393],\n",
      "        [12.2661],\n",
      "        [11.6742],\n",
      "        [13.6701],\n",
      "        [12.4350],\n",
      "        [12.5871],\n",
      "        [11.8920],\n",
      "        [13.9292],\n",
      "        [13.8132],\n",
      "        [12.6343],\n",
      "        [14.3710],\n",
      "        [13.1589],\n",
      "        [14.0663],\n",
      "        [13.1170],\n",
      "        [13.4524],\n",
      "        [12.5235],\n",
      "        [12.7461],\n",
      "        [11.2672],\n",
      "        [13.6123],\n",
      "        [12.6992],\n",
      "        [12.5597],\n",
      "        [11.1948],\n",
      "        [11.8474],\n",
      "        [12.2471],\n",
      "        [11.7898],\n",
      "        [12.5734],\n",
      "        [13.8464],\n",
      "        [13.1349],\n",
      "        [16.1225],\n",
      "        [13.5239],\n",
      "        [14.3423],\n",
      "        [13.5133],\n",
      "        [11.7638],\n",
      "        [12.2764],\n",
      "        [11.9335],\n",
      "        [13.7555],\n",
      "        [12.4885],\n",
      "        [12.1240],\n",
      "        [11.3727],\n",
      "        [11.8149],\n",
      "        [12.9061],\n",
      "        [13.1599],\n",
      "        [12.5112],\n",
      "        [12.3501],\n",
      "        [11.6351],\n",
      "        [11.6340],\n",
      "        [13.1139],\n",
      "        [12.6316],\n",
      "        [12.8466],\n",
      "        [12.2788],\n",
      "        [13.1069],\n",
      "        [12.7715],\n",
      "        [11.8917],\n",
      "        [12.4493],\n",
      "        [12.1640],\n",
      "        [11.6299],\n",
      "        [12.9125],\n",
      "        [11.1827],\n",
      "        [12.3938],\n",
      "        [11.4523],\n",
      "        [13.2697],\n",
      "        [11.5595],\n",
      "        [10.5292],\n",
      "        [12.0234],\n",
      "        [12.6085],\n",
      "        [12.1398],\n",
      "        [13.2504],\n",
      "        [11.5679],\n",
      "        [12.0671],\n",
      "        [11.9880],\n",
      "        [11.9272],\n",
      "        [13.1697],\n",
      "        [12.3325],\n",
      "        [13.1412],\n",
      "        [12.5955],\n",
      "        [13.7671],\n",
      "        [14.3924],\n",
      "        [12.2714],\n",
      "        [11.6265],\n",
      "        [11.9049],\n",
      "        [12.6896],\n",
      "        [10.8985],\n",
      "        [10.6399],\n",
      "        [12.7605],\n",
      "        [14.3239],\n",
      "        [12.8275],\n",
      "        [12.2335],\n",
      "        [12.7056],\n",
      "        [11.4644],\n",
      "        [12.3882],\n",
      "        [12.7347],\n",
      "        [11.0340],\n",
      "        [11.8292],\n",
      "        [13.7612],\n",
      "        [12.4066],\n",
      "        [12.7133],\n",
      "        [11.5454],\n",
      "        [11.4111],\n",
      "        [11.2667],\n",
      "        [10.2796],\n",
      "        [10.2399],\n",
      "        [14.0291],\n",
      "        [11.8282],\n",
      "        [11.7640],\n",
      "        [12.9138],\n",
      "        [10.5527],\n",
      "        [10.4128],\n",
      "        [10.8416],\n",
      "        [11.4833],\n",
      "        [11.0637],\n",
      "        [10.9179],\n",
      "        [12.2257],\n",
      "        [10.9433],\n",
      "        [10.9999],\n",
      "        [12.3251],\n",
      "        [10.9852],\n",
      "        [12.1264],\n",
      "        [12.0003],\n",
      "        [12.0398],\n",
      "        [11.1798],\n",
      "        [11.6288],\n",
      "        [13.2288],\n",
      "        [11.7716],\n",
      "        [11.2040],\n",
      "        [10.6018],\n",
      "        [10.8292],\n",
      "        [12.9268],\n",
      "        [10.1852],\n",
      "        [11.4293],\n",
      "        [10.9241],\n",
      "        [12.4570],\n",
      "        [10.9494],\n",
      "        [10.8555],\n",
      "        [11.9757],\n",
      "        [10.8853],\n",
      "        [11.7013],\n",
      "        [10.2896],\n",
      "        [10.8832],\n",
      "        [10.8188],\n",
      "        [10.3779],\n",
      "        [11.3375],\n",
      "        [ 9.8355],\n",
      "        [11.4364],\n",
      "        [11.0766],\n",
      "        [11.8735],\n",
      "        [10.1461],\n",
      "        [10.9304],\n",
      "        [10.3403],\n",
      "        [12.5324],\n",
      "        [11.7190],\n",
      "        [10.9695],\n",
      "        [11.0963],\n",
      "        [10.8557],\n",
      "        [10.9759],\n",
      "        [12.6301],\n",
      "        [12.3840],\n",
      "        [10.6907],\n",
      "        [10.6922],\n",
      "        [10.4267],\n",
      "        [10.0112],\n",
      "        [10.9333],\n",
      "        [11.1142],\n",
      "        [10.8098],\n",
      "        [11.8185],\n",
      "        [12.6313],\n",
      "        [10.8608],\n",
      "        [10.5895],\n",
      "        [ 9.0731],\n",
      "        [10.5616],\n",
      "        [11.4824],\n",
      "        [11.0059],\n",
      "        [11.4179],\n",
      "        [10.2773],\n",
      "        [10.5507],\n",
      "        [10.6443],\n",
      "        [ 9.3328],\n",
      "        [10.8026],\n",
      "        [10.1763],\n",
      "        [10.5817],\n",
      "        [10.1370],\n",
      "        [10.5743],\n",
      "        [ 9.5341],\n",
      "        [10.3034],\n",
      "        [10.5793],\n",
      "        [ 9.7699],\n",
      "        [10.7618],\n",
      "        [10.7935],\n",
      "        [ 9.0087],\n",
      "        [11.2355],\n",
      "        [11.2594],\n",
      "        [10.2621],\n",
      "        [10.2135],\n",
      "        [ 9.6792],\n",
      "        [10.2263],\n",
      "        [ 9.6512],\n",
      "        [10.6948],\n",
      "        [ 9.8044],\n",
      "        [11.2943],\n",
      "        [10.6145],\n",
      "        [ 9.5978],\n",
      "        [10.1380],\n",
      "        [11.0280],\n",
      "        [10.0470],\n",
      "        [11.6110],\n",
      "        [ 9.3092],\n",
      "        [11.2618],\n",
      "        [10.5190],\n",
      "        [10.6174],\n",
      "        [ 9.9494],\n",
      "        [10.2815],\n",
      "        [ 9.3751],\n",
      "        [10.9232],\n",
      "        [10.5887],\n",
      "        [10.6934],\n",
      "        [ 9.8700],\n",
      "        [ 9.8145],\n",
      "        [ 9.8066],\n",
      "        [ 8.9538],\n",
      "        [10.6087],\n",
      "        [10.2831],\n",
      "        [ 9.9844],\n",
      "        [ 9.5923],\n",
      "        [ 9.6038],\n",
      "        [ 9.8028],\n",
      "        [ 8.9356],\n",
      "        [ 8.8419],\n",
      "        [ 9.3689],\n",
      "        [11.0135],\n",
      "        [ 9.8032],\n",
      "        [ 9.3015],\n",
      "        [11.5661],\n",
      "        [10.0998],\n",
      "        [10.1708],\n",
      "        [10.3283],\n",
      "        [ 8.4162],\n",
      "        [ 9.2931],\n",
      "        [ 8.8149],\n",
      "        [ 9.5206],\n",
      "        [ 9.4794],\n",
      "        [ 9.4382],\n",
      "        [ 9.8554],\n",
      "        [ 8.6595],\n",
      "        [ 9.0078],\n",
      "        [ 8.6015],\n",
      "        [10.2908],\n",
      "        [ 9.5354],\n",
      "        [ 9.4146],\n",
      "        [ 9.4432],\n",
      "        [10.1359],\n",
      "        [ 8.9000],\n",
      "        [ 9.8958],\n",
      "        [ 9.4017],\n",
      "        [ 9.4817],\n",
      "        [ 8.4723],\n",
      "        [ 9.2424],\n",
      "        [ 9.4623],\n",
      "        [10.5083],\n",
      "        [ 8.9764],\n",
      "        [ 9.7220],\n",
      "        [ 9.7861],\n",
      "        [10.1251],\n",
      "        [ 8.9389],\n",
      "        [ 9.2488],\n",
      "        [ 9.5963],\n",
      "        [ 8.9501],\n",
      "        [ 8.7022],\n",
      "        [ 9.2802],\n",
      "        [ 8.7828],\n",
      "        [ 9.7735],\n",
      "        [ 9.6634],\n",
      "        [ 9.5551],\n",
      "        [ 9.8459],\n",
      "        [ 9.3863],\n",
      "        [ 8.7505],\n",
      "        [ 8.6757],\n",
      "        [ 8.9408],\n",
      "        [ 8.3308],\n",
      "        [ 8.7874],\n",
      "        [ 8.9058],\n",
      "        [ 8.8369],\n",
      "        [ 9.9219],\n",
      "        [ 8.5512],\n",
      "        [ 9.1191],\n",
      "        [ 8.3093],\n",
      "        [ 8.6716],\n",
      "        [ 8.8583],\n",
      "        [ 9.2156],\n",
      "        [ 9.0446],\n",
      "        [10.1875],\n",
      "        [11.5368],\n",
      "        [ 9.1302],\n",
      "        [ 8.6731],\n",
      "        [ 8.9999],\n",
      "        [ 8.8238],\n",
      "        [ 8.8783],\n",
      "        [ 9.1701],\n",
      "        [ 8.4079],\n",
      "        [ 8.3504],\n",
      "        [ 8.8725],\n",
      "        [ 8.6378],\n",
      "        [ 8.8974],\n",
      "        [ 9.1471],\n",
      "        [ 9.0728],\n",
      "        [ 8.8686],\n",
      "        [ 9.2171],\n",
      "        [ 8.8966],\n",
      "        [ 7.6095],\n",
      "        [ 8.9610],\n",
      "        [ 8.0403],\n",
      "        [ 8.2435],\n",
      "        [ 8.2567],\n",
      "        [ 8.6628],\n",
      "        [ 9.0121],\n",
      "        [ 8.3405],\n",
      "        [ 7.8658],\n",
      "        [ 8.7433],\n",
      "        [ 8.7452],\n",
      "        [ 8.3521],\n",
      "        [ 8.5350],\n",
      "        [ 9.1894],\n",
      "        [ 7.8067],\n",
      "        [ 8.2490],\n",
      "        [ 9.4624],\n",
      "        [12.0427],\n",
      "        [ 7.8316],\n",
      "        [ 9.3235],\n",
      "        [ 8.9109],\n",
      "        [ 8.3518],\n",
      "        [ 8.3211],\n",
      "        [ 8.5311],\n",
      "        [ 8.3975],\n",
      "        [ 8.4885],\n",
      "        [ 9.0921],\n",
      "        [ 7.6961],\n",
      "        [ 7.5672],\n",
      "        [ 8.2448],\n",
      "        [ 7.6900],\n",
      "        [ 8.2784],\n",
      "        [ 7.6019],\n",
      "        [ 8.2271],\n",
      "        [ 8.2471],\n",
      "        [ 9.0761],\n",
      "        [ 8.1772],\n",
      "        [ 8.1715],\n",
      "        [ 7.9759],\n",
      "        [ 8.8481],\n",
      "        [ 8.2950],\n",
      "        [ 7.7329],\n",
      "        [ 7.7173],\n",
      "        [ 8.1834],\n",
      "        [ 7.6403],\n",
      "        [ 8.4616],\n",
      "        [ 8.7825],\n",
      "        [ 7.9151],\n",
      "        [ 8.3433],\n",
      "        [ 8.7244],\n",
      "        [ 8.3672],\n",
      "        [ 7.2627],\n",
      "        [ 8.0536],\n",
      "        [ 7.0403],\n",
      "        [ 7.7639],\n",
      "        [ 8.2321],\n",
      "        [ 7.6958],\n",
      "        [ 7.9348],\n",
      "        [ 7.8319],\n",
      "        [ 7.6772],\n",
      "        [ 7.7941],\n",
      "        [ 7.9838],\n",
      "        [ 7.4816],\n",
      "        [ 8.0182],\n",
      "        [ 7.7989],\n",
      "        [ 8.3844],\n",
      "        [ 7.4332],\n",
      "        [ 7.6735],\n",
      "        [ 7.2303],\n",
      "        [ 7.4132],\n",
      "        [ 7.9104],\n",
      "        [ 7.8874],\n",
      "        [ 7.4631],\n",
      "        [ 8.2084],\n",
      "        [ 7.1818],\n",
      "        [ 7.6292],\n",
      "        [ 8.3286],\n",
      "        [ 7.4297],\n",
      "        [ 7.6277],\n",
      "        [ 7.3633],\n",
      "        [ 7.5527],\n",
      "        [ 6.9591],\n",
      "        [ 7.5313],\n",
      "        [ 7.1240],\n",
      "        [ 7.5198],\n",
      "        [ 7.1477],\n",
      "        [ 6.9938],\n",
      "        [ 6.7993],\n",
      "        [ 7.9228],\n",
      "        [ 7.7294],\n",
      "        [ 8.1719],\n",
      "        [ 7.4151],\n",
      "        [ 6.7884],\n",
      "        [ 7.2104],\n",
      "        [ 7.3969],\n",
      "        [ 7.5783],\n",
      "        [ 7.1447],\n",
      "        [ 7.3941],\n",
      "        [ 6.9078],\n",
      "        [ 7.0395],\n",
      "        [ 6.8352],\n",
      "        [ 7.4121],\n",
      "        [ 6.3258],\n",
      "        [ 7.2616],\n",
      "        [ 7.0280],\n",
      "        [ 7.1006],\n",
      "        [ 6.8563],\n",
      "        [ 7.0640],\n",
      "        [ 6.7395],\n",
      "        [ 7.3117],\n",
      "        [ 6.9304],\n",
      "        [ 6.9456],\n",
      "        [ 6.4346],\n",
      "        [ 7.6794],\n",
      "        [ 6.6338],\n",
      "        [ 6.7028],\n",
      "        [ 6.2137],\n",
      "        [ 6.0308],\n",
      "        [ 7.2804],\n",
      "        [ 6.8215],\n",
      "        [ 5.8129],\n",
      "        [ 6.7945],\n",
      "        [ 6.7591],\n",
      "        [ 6.5191],\n",
      "        [ 6.3423],\n",
      "        [ 6.1212],\n",
      "        [ 6.0742],\n",
      "        [ 6.6252],\n",
      "        [ 6.2326],\n",
      "        [ 6.1404],\n",
      "        [ 6.5661],\n",
      "        [ 6.4583],\n",
      "        [ 6.4144],\n",
      "        [ 6.1130],\n",
      "        [ 6.5050],\n",
      "        [ 6.4818],\n",
      "        [ 5.7338],\n",
      "        [ 5.8818],\n",
      "        [ 6.1686],\n",
      "        [ 6.1661],\n",
      "        [ 6.3497],\n",
      "        [ 5.8555],\n",
      "        [ 6.4556],\n",
      "        [ 6.4736],\n",
      "        [ 5.3118],\n",
      "        [ 5.8195],\n",
      "        [ 6.1111],\n",
      "        [ 6.5072],\n",
      "        [ 6.3726],\n",
      "        [ 6.0756],\n",
      "        [ 5.6766],\n",
      "        [ 5.5083],\n",
      "        [ 6.0310],\n",
      "        [ 5.9316],\n",
      "        [ 5.9996],\n",
      "        [ 6.1166],\n",
      "        [ 5.9781],\n",
      "        [ 5.8733],\n",
      "        [ 5.6567],\n",
      "        [ 5.7597],\n",
      "        [ 5.6748],\n",
      "        [ 5.8973],\n",
      "        [ 5.9613],\n",
      "        [ 6.2374],\n",
      "        [ 5.9923],\n",
      "        [ 5.5025],\n",
      "        [ 6.0122],\n",
      "        [ 5.6706],\n",
      "        [ 5.9968],\n",
      "        [ 5.6612],\n",
      "        [ 5.3520],\n",
      "        [ 5.2728],\n",
      "        [ 5.4217],\n",
      "        [ 5.9359],\n",
      "        [ 5.6398],\n",
      "        [ 6.0634],\n",
      "        [ 5.7190],\n",
      "        [ 5.6969],\n",
      "        [ 5.8967],\n",
      "        [ 5.6408],\n",
      "        [ 5.2832],\n",
      "        [ 5.7667],\n",
      "        [ 6.4474],\n",
      "        [ 5.4848],\n",
      "        [ 6.0867],\n",
      "        [ 6.1362],\n",
      "        [ 5.8768],\n",
      "        [ 6.5500],\n",
      "        [ 6.0299],\n",
      "        [ 5.8264],\n",
      "        [ 5.1772],\n",
      "        [ 5.2451],\n",
      "        [ 5.4263],\n",
      "        [ 5.5366],\n",
      "        [ 5.4949],\n",
      "        [ 5.0766],\n",
      "        [ 6.2501],\n",
      "        [ 4.9860],\n",
      "        [ 5.2329],\n",
      "        [ 5.1542],\n",
      "        [ 5.4654],\n",
      "        [ 5.0181],\n",
      "        [ 5.1821],\n",
      "        [ 5.6204],\n",
      "        [ 5.4792],\n",
      "        [ 5.2971],\n",
      "        [ 5.2994],\n",
      "        [ 4.9845],\n",
      "        [ 5.3909],\n",
      "        [ 5.4206],\n",
      "        [ 5.6774],\n",
      "        [ 5.2282],\n",
      "        [ 5.1912],\n",
      "        [ 4.9260],\n",
      "        [ 5.1664],\n",
      "        [ 5.0531],\n",
      "        [ 5.5934],\n",
      "        [ 5.3678],\n",
      "        [ 4.9417],\n",
      "        [ 4.9851],\n",
      "        [ 4.9637],\n",
      "        [ 5.0860],\n",
      "        [ 4.9264],\n",
      "        [ 5.0251],\n",
      "        [ 4.9417],\n",
      "        [ 5.1355],\n",
      "        [ 4.1949],\n",
      "        [ 5.0124],\n",
      "        [ 4.5859],\n",
      "        [ 5.0862],\n",
      "        [ 4.5982],\n",
      "        [ 4.6908],\n",
      "        [ 4.8074],\n",
      "        [ 4.8976],\n",
      "        [ 4.8070],\n",
      "        [ 4.8618],\n",
      "        [ 4.6690],\n",
      "        [ 5.0532],\n",
      "        [ 4.6505],\n",
      "        [ 4.8112],\n",
      "        [ 4.6232],\n",
      "        [ 4.1234],\n",
      "        [ 4.6767],\n",
      "        [ 4.4554],\n",
      "        [ 5.1396],\n",
      "        [ 4.7448],\n",
      "        [ 4.5001],\n",
      "        [ 4.3694],\n",
      "        [ 4.4883],\n",
      "        [ 4.4489],\n",
      "        [ 4.3065],\n",
      "        [ 4.3386],\n",
      "        [ 4.5020],\n",
      "        [ 4.5539],\n",
      "        [ 4.2377],\n",
      "        [ 4.2371],\n",
      "        [ 4.2143],\n",
      "        [ 4.2964],\n",
      "        [ 4.4235],\n",
      "        [ 4.1981],\n",
      "        [ 4.2088],\n",
      "        [ 4.1312],\n",
      "        [ 4.0998],\n",
      "        [ 4.5238],\n",
      "        [ 3.9176],\n",
      "        [ 4.0914],\n",
      "        [ 3.7720],\n",
      "        [ 4.0840],\n",
      "        [ 3.9131],\n",
      "        [ 3.7081],\n",
      "        [ 3.9010],\n",
      "        [ 4.0216],\n",
      "        [ 3.7595],\n",
      "        [ 3.7778],\n",
      "        [ 3.4758],\n",
      "        [ 3.9256],\n",
      "        [ 3.8454],\n",
      "        [ 3.5121],\n",
      "        [ 3.6674],\n",
      "        [ 3.6541],\n",
      "        [ 3.4280],\n",
      "        [ 3.6862],\n",
      "        [ 3.8986],\n",
      "        [ 3.6642],\n",
      "        [ 3.5593],\n",
      "        [ 3.5948],\n",
      "        [ 3.8086],\n",
      "        [ 3.5398],\n",
      "        [ 3.3638],\n",
      "        [ 3.3687],\n",
      "        [ 3.2543],\n",
      "        [ 3.4394],\n",
      "        [ 3.3286],\n",
      "        [ 3.2932],\n",
      "        [ 3.2354],\n",
      "        [ 3.2199],\n",
      "        [ 3.2611],\n",
      "        [ 3.3994],\n",
      "        [ 3.0158],\n",
      "        [ 3.2122],\n",
      "        [ 3.0269],\n",
      "        [ 3.0396],\n",
      "        [ 2.7105],\n",
      "        [ 2.9152],\n",
      "        [ 2.8116],\n",
      "        [ 2.8462],\n",
      "        [ 2.9581],\n",
      "        [ 2.9747],\n",
      "        [ 2.6615],\n",
      "        [ 2.7692],\n",
      "        [ 2.7103],\n",
      "        [ 2.5225],\n",
      "        [ 2.4555],\n",
      "        [ 2.5238],\n",
      "        [ 2.4583],\n",
      "        [ 2.4564],\n",
      "        [ 2.4378],\n",
      "        [ 2.3594],\n",
      "        [ 2.3240],\n",
      "        [ 2.2792],\n",
      "        [ 1.9962],\n",
      "        [ 2.1629],\n",
      "        [ 2.0261],\n",
      "        [ 1.9703],\n",
      "        [ 2.3539],\n",
      "        [ 1.8160],\n",
      "        [ 1.6803],\n",
      "        [ 1.8129],\n",
      "        [ 1.6945],\n",
      "        [ 1.7267],\n",
      "        [ 1.7723],\n",
      "        [ 1.5667],\n",
      "        [ 1.4688],\n",
      "        [ 1.2756],\n",
      "        [ 0.7513]])\n"
     ]
    }
   ],
   "source": [
    "# se utilizará un perceptrón que va permitir aproximar el modelo por medio de descenso del gradiente\n",
    "\n",
    "n_epochs = 50\n",
    "w = torch.rand(6,1)\n",
    "\n",
    "# se define el learning rate\n",
    "lr = 0.1\n",
    "\n",
    "SamplesAll = SamplesAll.to(torch.float32)\n",
    "w = w.to(torch.float32)\n",
    "\n",
    "# por cada iteración se actualiza el valor de pesos\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = SamplesAll.mm(w)\n",
    "\n",
    "    error = targetsAll - yhat\n",
    "\n",
    "    # la actualización depende del gradiente del error, entre más cercano a cero puede \n",
    "    #    actualizar el valor\n",
    "    w_grad = -2  * error.mean()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w_grad\n",
    "\n",
    "# Se imprimen los resultados para comparar\n",
    "\n",
    "print(yhat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Realice al menos tres conclusiones sobre el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La aproximación manual del modelo requiere de más código que la que utiliza los módulos de pytorch.\n",
    "- Otra opción para realizar regresión corresponde a la técnica de descenso de gradiente.\n",
    "- La forma de comprobar si el modelo está bien entrenado corresponde a añadir valores de entrada conocidos y utilizar el perceptrón para definir si el resultado estimado se acerca al real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias**\n",
    "\n",
    "[1] 'Exploración y vusualización de datos', notas de clase del curso IC-6200, Sede Interuniversitaria de Alajuela, I Semestre del 2022.\n",
    "\n",
    "[2] 'Regresión no lineal', notas de clase del curso IC-6200, Sede Interuniversitaria de Alajuela, I Semestre del 2022.\n",
    "\n",
    "[3] 'Regresión lineal', notas de clase del curso IC-6200, Sede Interuniversitaria de Alajuela, I Semestre del 2022.\n",
    "\n",
    "[4] Pytorch (2022) Pythorch Documentation. Disponible en: https://pytorch.org/docs/stable/index.html\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60900aa771768904a6d6616df638e60e3e0400762e431fc6f8b11a1c64e30e12"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

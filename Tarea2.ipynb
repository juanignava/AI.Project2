{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instituto Tecnol贸gico de Costa Rica (ITCR)\n",
    "### Escuela de Computaci贸n\n",
    "### Curso: Inteligencia Artificial\n",
    " \n",
    "### Segunda tarea programada 2022-I\n",
    "\n",
    "\n",
    "Estudiantes: Juan Ignacio Navarro Navarro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 1. Graficaci贸n\n",
    "\n",
    "---\n",
    "\n",
    "Para las siguientes:\n",
    "\n",
    "Funci贸n 1:\n",
    "\n",
    "$f_{1}\\left(x_{1},x_{2}\\right)=\\left(x_{1}-0.7\\right)^{2}+\\left(x_{2}-0.5\\right)^{2}$\n",
    "\n",
    "\n",
    "Funci贸n 2: \n",
    "\n",
    "$f_{2}\\left(x_{1},x_{2}\\right)=x_{1}e^{\\left(-x_{1}^{2}-x_{2}^{2}\\right)}$\n",
    "\n",
    "\n",
    "Realice lo siguiente (5 puntos):\n",
    "\n",
    "En Python, cree una funci贸n que le permita graficar las funciones anteriores. Utilizando la funci贸n en Python genere dos gr谩ficos, uno para cada funci贸n $_{1}$ y  $_{2}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import exp,arange\n",
    "from pylab import meshgrid,cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Se define la funcion que se va a graficar\n",
    "\n",
    "def func_1(x1,x2):\n",
    "    \"\"\"\n",
    "    Este m茅todo define el resultado de la funci贸n 1\n",
    "    del enunciado dados dos valores x1 y x2\n",
    "    \"\"\"\n",
    "    return (x1 - 0.7)**2 + (x2 - 0.5)**2\n",
    "\n",
    "\n",
    "def func_2(x1, x2):\n",
    "    \"\"\"\n",
    "    Este m茅todo define el resultado de la funci贸n 2\n",
    "    del enunciado dados dos valores x1 y x2\n",
    "    \"\"\"\n",
    "    return x1 * exp(-x1**2 - x2**2)\n",
    "\n",
    "def grafica3D(X,Y,Z):\n",
    "    \"\"\"\n",
    "    Esta fuci贸n realiza la gr谩fica una funci贸n con dos variables\n",
    "    independientes X, Y y una dependiente Z\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig)\n",
    "\n",
    "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.RdBu,linewidth=0, antialiased=False)\n",
    "\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Definici贸n del rango de valores a graficar\n",
    "x = arange(-2.0,2.0,0.1)\n",
    "y = arange(-2.0,2.0,0.1)\n",
    "\n",
    "#Se define la grilla de puntos para x y y\n",
    "X,Y = meshgrid(x, y)\n",
    "\n",
    "# Se evalua la primera funcion 1 segun los valores de X y Y\n",
    "Z = func_1(X, Y)\n",
    "\n",
    "# Se grafica la primera funci贸n 1\n",
    "grafica3D(X,Y,Z)\n",
    "\n",
    "# Se evalua la segunda funcion 2 con los valores de X y Y \n",
    "Z = func_2(X, Y)\n",
    "\n",
    "# Se grafica la primera funcion 2\n",
    "grafica3D(X, Y, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 2. Optimizaci贸n\n",
    "---\n",
    "\n",
    "**a) Vector gradiente  (5 puntos)**\n",
    "\n",
    "Calcule el vector gradiente $\\nabla f$ para la siguiente funci贸n multi-variable $f: \\mathbb{R}^2\\rightarrow\\mathbb{R}$. Adem谩s, eval煤elo en $\\begin{bmatrix}1\\\\1\\end{bmatrix}$ y $\\begin{bmatrix}-1\\\\-1\\end{bmatrix}$. \n",
    "\n",
    "--No se require programaci贸n en python--.\n",
    "\n",
    " - $f_3(x,y) = x^4+y^3+5x^2y^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuesta\n",
    "\n",
    "El gradiente de la funci贸n $f_3$ es:\n",
    "\n",
    "$$\\nabla _{f_3}(x, y) = (4x^3 + 10xy^3, 3y^2+ 15x^2y^2)$$\n",
    "\n",
    "Al evaluar el vector en $\\begin{bmatrix}1\\\\1\\end{bmatrix}$ se obtiene:\n",
    "\n",
    "$$\\nabla _{f_3}(1, 1) = (4 + 10, 3 + 15)$$\n",
    "$$\\nabla _{f_3}(1, 1) = (14, 18)$$\n",
    "\n",
    "Ahora al evaluarlo en $\\begin{bmatrix}-1\\\\-1\\end{bmatrix}$:\n",
    "\n",
    "$$\\nabla _{f_3}(-1, -1) = (-4 + 10, 3 + 15)$$\n",
    "$$\\nabla _{f_3}(-1, -1) = (6, 18)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Descenso de gradiente (10 puntos)**\n",
    "\n",
    "**Sea la funci贸n:** \n",
    "\n",
    "\\begin{equation}\n",
    "f_4\\left(\\overrightarrow{x}\\right)=(x-0.7)^{2}+(y-0.5)^{2},\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Implemente el algoritmo de descenso de gradiente para $f_4(x)$ en la funci贸n en Python denominada: (8 puntos)**\n",
    "\n",
    "$$funcion\\_gradient\\_descent \\left(learning\\_rate, max\\_iters, starting\\_point, f\\_function,f\\_gradient, precision\\right)$$\n",
    "\n",
    "donde los par谩metros corresponden a:\n",
    "\n",
    "* learning_rate: tasa_aprendizaje o el $\\alpha$\n",
    "* max_iters: es el m谩ximo n煤mero de iteraciones a ejecutar\n",
    "* starting_point: es el vector con los dos valores iniciales [x,y]\n",
    "* f_function: fuci贸n a optimizar\n",
    "* f_gradient: gradiente de la funci贸n a optimizar.\n",
    "* precision: es el valor m铆nimo para un cambio entre iteraci贸n. \n",
    "\n",
    "Despliegue y analice los resultados (2 puntos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_4(x, y):\n",
    "    \"\"\"\n",
    "    Codigo de la funci贸n 4 definida en el enunciado\n",
    "    \"\"\"\n",
    "    return (x - 0.7)^2 + (y - 0.5)^2\n",
    "\n",
    "def gfunc_4(x, y):\n",
    "    \"\"\"\n",
    "    Gradiente de la funci贸n 4 definida en el enunciado\n",
    "    \"\"\"\n",
    "    return 2*(x - 0.7), 2*(y - 0.5)\n",
    "\n",
    "def function_gradient_descent(learning_rate, max_iters, starting_point, f_function, f_gradient, precision):\n",
    "    \"\"\"\n",
    "    Esta funci贸n calcula el restultado del algoritmo de descenso de gradiente\n",
    "    \"\"\"\n",
    "    current_value = starting_point\n",
    "    grad_norm = 1\n",
    "    iters = 0\n",
    "\n",
    "    # iteracion hasta que la noram del gradiente sea menos a la precision indicada\n",
    "    while grad_norm > precision and iters < max_iters:\n",
    "        curr_x = current_value[0]\n",
    "        curr_y = current_value[1]\n",
    "        curr_grad = f_gradient(curr_x, curr_y)\n",
    "        prev_value = current_value\n",
    "        # calculo del nuevo valor: x = x - k * gradiente( f(x) )\n",
    "        current_value = prev_value[0] - learning_rate * curr_grad[0] , prev_value[1] - learning_rate * curr_grad[1]\n",
    "        # actualizaci贸n de la norma del gradiente\n",
    "        grad_norm = (curr_grad[0]**2 + curr_grad[1]**2)**0.5\n",
    "        iters += 1\n",
    "\n",
    "    return current_value\n",
    "\n",
    "# definicion de las variables iniciales\n",
    "learning_rate = 0.01\n",
    "max_iters = 10000\n",
    "starting_point = (3, 2)\n",
    "f_function = func_4\n",
    "f_gradient = gfunc_4\n",
    "precision = 0.000001\n",
    "\n",
    "# como resultado se imprime el valor del descenso de gradiente\n",
    "print(function_gradient_descent(learning_rate, max_iters, starting_point, f_function, f_gradient, precision))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 3. An谩lisis de regresi贸n con aprendizaje profundo (con PyTorch)\n",
    "\n",
    "Para realizar el an谩lisis se utilizar谩 un conjunto de datos generado por la Facultad de Ciencias de la Informaci贸n y la Computaci贸n Donald Bren de la Universidad de California en Irvine disponibles en https://www.kaggle.com/rodolfomendes/abalone-dataset (copia adjunta).   \n",
    "\n",
    "Descripci贸n de los datos:\n",
    "El conjunto de datos puede ser utilizado para entrenar modelos para predecir la edad de los abulones (moluscos tambi茅n conocidos como orejas de mar) a partir de mediciones f铆sicas. Com煤nmente, la edad de un abul贸n se determina cortando la concha a trav茅s del cono, ti帽茅ndola y contando el n煤mero de anillos a trav茅s de un microsc贸pio, una tarea que requiere mucho tiempo. Sin embargo, es posible utilizar datos morfol贸gicos del individuo, que son m谩s f谩ciles de obtener y permiten predecir la edad este. Una descripci贸n detallada de los datos est谩 disponible en https://archive.ics.uci.edu/ml/datasets/abalone. \n",
    "\n",
    "Dado el conjunto de datos de abulones se **desea crear un modelo de regresi贸n utilizando un perceptr贸n multicapa para predecir la cantidad de anillos (columna Rings) de estos a partir del conjunto de caracter铆sticas**.\n",
    "\n",
    "Realice lo siguiente:\n",
    "\n",
    "(requisito indispensable para tomar en cuenta el ejercicio, deben usar PyTorch).\n",
    "- a) (1 punto) Describa el conjunto de datos, cada uno de sus campos y referencie la fuente. \n",
    "- b) Cargue el conjunto de datos.\n",
    "- c) (3 puntos) Explore y limpie el conjunto de datos, visualice algunas estad铆sticas, presente una matriz de calor y verifique que no existan valores faltantes.\n",
    "- d) (5 puntos) Defina el modelo utilizando un perceptr贸n multicapa implementado con PyTorch.\n",
    "- e) (5 puntos) Realice el entrenamiento del modelo.\n",
    "- f) (5 puntos) Calcule la p茅rdida utilizando el error cuadr谩tico medio.\n",
    "- g) (2 puntos) Prepare un ejemplo de uso del modelo seleccionado y explique el resultado.\n",
    "- h) (5 puntos) Realice al menos tres conclusiones sobre el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)\n",
    "\n",
    "Descripci贸n de los datos:\n",
    "\n",
    "- Lenght: corresponde a la longitud m谩s larga de la concha en mil铆metros.\n",
    "- Diameter: al diametro en milimetros perpendicular a la longitud.\n",
    "- Height: Es la altura en milimetros con carne en la concha.\n",
    "- Whole: Es el peso total en gramos de todo el abal贸n.\n",
    "- Viscera: Es el peso en gramos de la carne.\n",
    "- Shellweight: Es el peso de la concha en gramos despu茅s de ser secada.\n",
    "- Rings: es un entero que corresponde a la cantidad de anillos. Es 1.5 m谩s en adici贸n a la cantidad de a帽os del abal贸n.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)\n",
    "- Carga de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas requeridas para el ejercicio\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch as torch\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de los datos de los abalones\n",
    "\n",
    "data = pd.read_csv('datos/Abalone.csv')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c)\n",
    "- Exploraci贸n de los datos y mapa de calor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribuci贸n de los anillos con respecto a la longitud\n",
    "\n",
    "def plot_data(df, col_x, col_y, label_x, label_y, val_title):\n",
    "    # Imprime gr谩fica de dispersi贸n. \n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.scatter(x=df[col_x],y=df[col_y])\n",
    "    plt.xlabel(label_x)\n",
    "    plt.ylabel(label_y)\n",
    "    plt.title(val_title)\n",
    "    plt.show()\n",
    "    \n",
    "plot_data(data,'Length', 'Rings', 'Longitud de la concha','Anillos',\n",
    "          'Longitud de la concha - Relaci贸n anillos')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note que hay datos que est谩n un poco perdidos por lo que se debe\n",
    "#  realizar una limpieza en el modelo, esto se hace a continuaci贸n\n",
    "\n",
    "data = data[data.Rings <= 100]\n",
    "data = data[data.Rings >= -100]\n",
    "\n",
    "plot_data(data,'Length', 'Rings', 'Longitud de la concha','Anillos',\n",
    "          'Longitud de la concha - Relaci贸n anillos')\n",
    "\n",
    "# Note que al correr el modelo ahora los datos se ven mejor \n",
    "#  sin el ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribuci贸n de los anillos con respecto al diametro\n",
    "    \n",
    "plot_data(data,'Diameter', 'Rings', 'Diametro de la concha','Anillos',\n",
    "          'Diametro de la concha - Relaci贸n anillos')\n",
    "\n",
    "# En este caso no se observan valores perdidos por lo que no hace\n",
    "# falta la limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribuci贸n de los anillos con respecto a la altura\n",
    "    \n",
    "plot_data(data,'Height', 'Rings', 'Altura de la concha','Anillos',\n",
    "          'Altura de la concha - Relaci贸n anillos')\n",
    "\n",
    "# En este caso no se observan valores perdidos por lo que no hace\n",
    "# falta la limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribuci贸n de los anillos con respecto al peso completo\n",
    "    \n",
    "plot_data(data,'Whole', 'Rings', 'Peso de todo el abal贸n','Anillos',\n",
    "          'Peso de todo el abal贸n - Relaci贸n anillos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este caso se podr铆a hacer una peque帽a limpieza para apreciar\n",
    "# mejor los valores acomulados\n",
    "\n",
    "data = data[data.Whole <= 2]\n",
    "\n",
    "plot_data(data,'Whole', 'Rings', 'Peso de todo el abal贸n','Anillos',\n",
    "          'Peso de todo el abal贸n - Relaci贸n anillos')\n",
    "\n",
    "# Note que ahora se ve una mejor distribuci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribuci贸n de los anillos con respecto al peso de la viscera\n",
    "# del abal贸n\n",
    "    \n",
    "plot_data(data,'Viscera', 'Rings', 'Peso de la viscera del abal贸n','Anillos',\n",
    "          'Peso de la viscera del abal贸n - Relaci贸n anillos')\n",
    "\n",
    "# aprovechamos para realizar un peque帽o corte\n",
    "\n",
    "data = data[data.Viscera <= 0.4]\n",
    "\n",
    "plot_data(data,'Whole', 'Rings', 'Peso de todo el abal贸n','Anillos',\n",
    "          'Peso de todo el abal贸n - Relaci贸n anillos')\n",
    "\n",
    "# Ahora se ve un poco mejor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribuci贸n de los anillos con respecto al peso de la concha\n",
    "# del abal贸n\n",
    "    \n",
    "plot_data(data,'Shellweight', 'Rings', 'Peso de la concha del abal贸n','Anillos',\n",
    "          'Peso de la concha del abal贸n - Relaci贸n anillos')\n",
    "\n",
    "# aprovechamos para realizar un peque帽o corte\n",
    "\n",
    "data = data[data.Shellweight <= 0.5]\n",
    "\n",
    "plot_data(data,'Shellweight', 'Rings', 'Peso de la concha del abal贸n','Anillos',\n",
    "          'Peso de la concha del abal贸n - Relaci贸n anillos')\n",
    "\n",
    "# Ahora se ve un poco mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad铆ticas de los datos\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La siguiente gr谩fica presenta relaci贸n entre pares de variables.\n",
    "\n",
    "# En la diagonal se ven los histogramas de cada variables\n",
    "# La segunda fila presenta la ultima fila presenta la relaci贸n de la variable Anillos\n",
    "# con todas las dem谩s\n",
    "\n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlaci贸n de los datos\n",
    "\n",
    "# A continuaci贸n se muestra la correlaci贸n de los datos por medio del mapa de calor\n",
    "\n",
    "# Se puede observar que los datos est谩n muy relacionados entre s铆 a excepci贸n de \n",
    "# las relaciones con los anillos. \n",
    "\n",
    "def correlation_heatmap(df1):\n",
    "    _,ax=plt.subplots(figsize=(15,10))\n",
    "    colormap=sns.diverging_palette(220,10,as_cmap=True)\n",
    "    sns.heatmap(data.corr(),annot=True,cmap=colormap)\n",
    "    \n",
    "correlation_heatmap(data)\n",
    "\n",
    "# Note que en la mayor铆a los valores est谩n altamente correlacionados\n",
    "# En realidad la 煤ltima columna/fila (tiene los mismos valores) \n",
    "# de esta matriz es la que m谩s nos interesa, es la correlaci贸n \n",
    "# de todas las gr谩ficas que vimos anteriormente. Note que las variables\n",
    "# que tienen m谩s correlaci贸n ser谩n las que probablemente tengan \n",
    "# m谩s peso en el modelo de predicci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) \n",
    "Defina el modelo utilizando un perceptr贸n multicapa implementado con PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principales bibliotecas que utilizar谩n en el perceptr贸n\n",
    "from numpy import vstack\n",
    "from numpy import sqrt\n",
    "from numpy import array\n",
    "from pandas import read_csv, notnull\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.init import xavier_uniform_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo corresponde al siguiente c贸digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES\n",
    "\n",
    "# definicion del dataset\n",
    "class CSVDataset(Dataset):\n",
    "    # carga del dataset\n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        The __init__ function is run once when instantiating the Dataset object. \n",
    "        :param path: the path and name of the file to process. \n",
    "        \"\"\"\n",
    "        # load the csv file as a dataframe\n",
    "        df = pd.read_csv(path)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1].astype('float32')\n",
    "        self.y = df.values[:, -1].astype('float32')\n",
    "        # ensure target has the right shape\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The __len__ function returns the number of samples in our dataset.\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The __getitem__ function loads and returns \n",
    "        a sample from the dataset at the given index idx. \n",
    "        :param idx: index.\n",
    "        \"\"\"\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        \"\"\"\n",
    "        Split the dataset into training and test data.\n",
    "        :param n_test: training data percentage\n",
    "        \"\"\"\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "    \n",
    "def prepare_data(path):\n",
    "    \"\"\"\n",
    "    Prepare the dataset.\n",
    "    :param path: path and name of the file .\n",
    "    \"\"\"\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "\n",
    "def evaluate_model(test_dl, model):\n",
    "    \"\"\"\n",
    "    Evaluates the model performance using Mean Squared Error (MSE).\n",
    "    :param: test_dt: test data.\n",
    "    :param: model: model to evaluate.\n",
    "    \"\"\"\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate mse\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def predict(row, model):\n",
    "    \"\"\"\n",
    "    Make a class prediction for one row of data\n",
    "    \"\"\"\n",
    "\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class MLP(Module):\n",
    "    \"\"\"\n",
    "    Class that implements the perceptron, it extends the Module class.\n",
    "    \"\"\"\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs, n_output):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 4)\n",
    "        # Values are scaled by the gain parameter using a uniform distribution.\n",
    "        # No gradient will be recorded for this operation.\n",
    "        xavier_uniform_(self.hidden1.weight)\n",
    "        self.act1 = Sigmoid()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(4, 2)\n",
    "        xavier_uniform_(self.hidden2.weight)\n",
    "        self.act2 = Sigmoid()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(2, n_output)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward run of the network using the data in X.\n",
    "        \"\"\"\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "Realice el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, model):\n",
    "    \"\"\"\n",
    "    Train the model using the train data loader (train_dl).\n",
    "    \"\"\"\n",
    "    \n",
    "    # define the optimization\n",
    "    # Mean Squared Error (MSE)\n",
    "    criterion = MSELoss()\n",
    "    # Stochastic gradient descent (SGD)\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661 326\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001CB40859B80>\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "path = 'datos/Abalone.csv'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "print(train_dl)\n",
    "\n",
    "# define the network\n",
    "n_inputs = 6\n",
    "n_output = 1\n",
    "model = MLP( n_inputs, n_output)\n",
    "\n",
    "# train the model\n",
    "train_model(train_dl, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)\n",
    "\n",
    "Calcule la p茅rdida usando el error cuadr谩tico medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 255593.797, RMSE: 505.563\n"
     ]
    }
   ],
   "source": [
    "# Evaluaci贸n del modelo usando el error cuadr谩tico medio (MSE)\n",
    "mse = evaluate_model(test_dl, model)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))\n",
    "\n",
    "# Note que al correrlo se obtiene un error cuadr谩tico medio bajo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g)\n",
    "\n",
    "Prepare un modelo del ejemplo determinado y explique los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicci贸n del modelo: 17.063\n",
      "Valor obtenido de los datos: 17\n"
     ]
    }
   ],
   "source": [
    "# Realizamos una predicci贸n con el modelo con la primera\n",
    "#  fila de datos del documento del Abal贸n, cuyo valor\n",
    "#  esperado corresponde a 17 anillos\n",
    "row = [0.745,0.585,0.215,2.499,0.472,0.7]\n",
    "yhat = predict(row, model)\n",
    "print('Predicci贸n del modelo: %.3f' % yhat)\n",
    "print('Valor obtenido de los datos: 17')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que seg煤n los resultados, el modelo se acerca bastante a lo esperado. El resultado no es igual pero cercano incluso hasta redodeado al entero m谩s cercano se obtiene lo deseado. Gracias a este modelo se podr铆an obtener otros datos de abal贸n y a partir de estos conseguir la cantidad de anillos sin necesidad del experimento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h)\n",
    "\n",
    "Realice 3 conclusiones sobre el ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los perceptrones multicapa permiten pasar los datos por varios niveles de regresi贸n y as铆 mejorar la precisi贸n del modelo.\n",
    "- Los modelos de perceptr贸n multicapa se pueden adaptar a cualquier caso de conjutno de datos que tengan una dependencia entre s铆, a pesar de que la dependencia no es trivial el modelo logra encontrar una r谩pidamente.\n",
    "- Gracias a la utilizaci贸n de este modelo se obteiene una forma sencilla de obtener la edad de Abal贸n con base en datos datos mucho m谩s sencillos de medir, esta facilidad acelera considerablemente el an谩lisis de los pr贸ximos Abalones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 4. Ajuste de curvas con m铆nimos cuadrados.\n",
    "(requisito indispensable para tomar en cuenta el ejercicio, deben usar tensores de PyTorch).\n",
    "- a) (5 puntos) Calcule el $w_{opt}$ (el w 贸ptimo) para los datos de los abulones. \n",
    "- b) (5 puntos) Implemente la funci贸n forward, la cual estima las salidas del modelo al hacer  $f(x) =X\\,\\vec{w}_{opt}$ donde la funci贸n f(x) se refiere a la funci贸n de activaci贸n, con X la matriz de caracter铆sticas.\n",
    "- c) (5 puntos) Calcule la p茅rdida utilizando el error cuadr谩tico medio.\n",
    "- d) (3 puntos) Realice al menos tres conclusiones sobre el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "Calcule el $w_{opt}$ (el w 贸ptimo) para los datos de los abulones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor optimo:  tensor([[13.1107],\n",
      "        [-1.6755],\n",
      "        [33.6175],\n",
      "        [-5.4968],\n",
      "        [-9.9780],\n",
      "        [28.0774]])\n"
     ]
    }
   ],
   "source": [
    "# Se estima el W 贸ptimo con m铆nimos cuadrados\n",
    " \n",
    "def estimateOptimumW(targetsAll, SamplesAll):\n",
    "    \"\"\" \n",
    "    Estimate the optimum W with least squares\n",
    "    param TargetsAll, target\n",
    "    param SamplesAll, NumSamples \n",
    "    return \n",
    "       wOpt, array with optimum weights\n",
    "    \"\"\"\n",
    "    # Calculate w = Apinverse*Targets\n",
    "    samplesAllPinv = torch.tensor(np.linalg.pinv(SamplesAll))\n",
    "    samplesAllPinv = samplesAllPinv.to(torch.float32)\n",
    "    targetsAll = targetsAll.to(torch.float32)\n",
    "    wOpt = samplesAllPinv.mm(targetsAll)\n",
    "    return wOpt\n",
    " \n",
    "# La informaici贸n se convierte en arrays de numpy\n",
    "# para despu茅s pasarlos a tensores de Pytorch\n",
    "SamplesAll = torch.tensor(data.iloc[:,[0,1,2,3,4,5]].to_numpy())\n",
    "targetsAll = torch.tensor(data.iloc[:,[6]].to_numpy())\n",
    "\n",
    "# The targets are adjusted to be able to computet he multiplication. \n",
    "targetsAll=targetsAll.reshape([targetsAll.shape[0],1])\n",
    "\n",
    "# Se calcula el w 贸ptimo\n",
    "wOpt = estimateOptimumW( targetsAll, SamplesAll)\n",
    "\n",
    "print(\"El valor optimo: \", wOpt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "Implemente la funci贸n forward, la cual estima las salidas del modelo al hacer  $f(x) =X\\,\\vec{w}_{opt}$ donde la funci贸n f(x) se refiere a la funci贸n de activaci贸n, con X la matriz de caracter铆sticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15.3877],\n",
      "        [14.6100],\n",
      "        [16.5489],\n",
      "        [16.0999],\n",
      "        [14.1067],\n",
      "        [14.0875],\n",
      "        [13.8591],\n",
      "        [14.6315],\n",
      "        [12.8525],\n",
      "        [15.2496],\n",
      "        [13.2011],\n",
      "        [13.2040],\n",
      "        [14.2062],\n",
      "        [17.1331],\n",
      "        [12.6125],\n",
      "        [13.4489],\n",
      "        [14.8094],\n",
      "        [14.6450],\n",
      "        [12.2912],\n",
      "        [15.1556],\n",
      "        [13.0127],\n",
      "        [15.4105],\n",
      "        [17.8852],\n",
      "        [15.3657],\n",
      "        [16.8495],\n",
      "        [14.2335],\n",
      "        [15.9262],\n",
      "        [14.3349],\n",
      "        [16.4405],\n",
      "        [13.8627],\n",
      "        [15.4082],\n",
      "        [14.5187],\n",
      "        [15.6603],\n",
      "        [13.9832],\n",
      "        [16.1309],\n",
      "        [15.2123],\n",
      "        [12.7525],\n",
      "        [13.8879],\n",
      "        [13.4967],\n",
      "        [14.9629],\n",
      "        [11.2373],\n",
      "        [12.5461],\n",
      "        [14.7058],\n",
      "        [17.0341],\n",
      "        [15.0235],\n",
      "        [17.4306],\n",
      "        [14.6517],\n",
      "        [10.6152],\n",
      "        [15.9025],\n",
      "        [12.7010],\n",
      "        [15.1452],\n",
      "        [15.4699],\n",
      "        [12.8616],\n",
      "        [13.6418],\n",
      "        [10.0686],\n",
      "        [10.9678],\n",
      "        [11.3623],\n",
      "        [13.4009],\n",
      "        [11.7455],\n",
      "        [15.9022],\n",
      "        [15.2332],\n",
      "        [14.9012],\n",
      "        [16.1615],\n",
      "        [14.7238],\n",
      "        [13.3640],\n",
      "        [11.9981],\n",
      "        [14.4838],\n",
      "        [14.6647],\n",
      "        [15.2307],\n",
      "        [14.6851],\n",
      "        [14.7384],\n",
      "        [13.7520],\n",
      "        [15.9803],\n",
      "        [12.5993],\n",
      "        [14.5953],\n",
      "        [13.4942],\n",
      "        [16.8056],\n",
      "        [14.2625],\n",
      "        [15.8645],\n",
      "        [14.3512],\n",
      "        [14.6183],\n",
      "        [14.0089],\n",
      "        [15.8174],\n",
      "        [16.8480],\n",
      "        [13.1624],\n",
      "        [12.2915],\n",
      "        [17.6767],\n",
      "        [12.4924],\n",
      "        [13.4735],\n",
      "        [14.2450],\n",
      "        [16.7124],\n",
      "        [16.7438],\n",
      "        [15.1903],\n",
      "        [11.2606],\n",
      "        [12.7196],\n",
      "        [11.9585],\n",
      "        [11.9815],\n",
      "        [14.2589],\n",
      "        [13.9014],\n",
      "        [14.0626],\n",
      "        [14.7603],\n",
      "        [16.2013],\n",
      "        [14.1754],\n",
      "        [10.4452],\n",
      "        [11.9370],\n",
      "        [10.8045],\n",
      "        [14.1011],\n",
      "        [12.9505],\n",
      "        [14.9876],\n",
      "        [13.2584],\n",
      "        [11.0519],\n",
      "        [11.0300],\n",
      "        [13.4317],\n",
      "        [13.5680],\n",
      "        [14.6228],\n",
      "        [13.1576],\n",
      "        [11.7781],\n",
      "        [14.1372],\n",
      "        [13.6537],\n",
      "        [13.3599],\n",
      "        [13.9920],\n",
      "        [14.3085],\n",
      "        [13.9720],\n",
      "        [13.3810],\n",
      "        [14.9376],\n",
      "        [14.5005],\n",
      "        [15.1560],\n",
      "        [13.4496],\n",
      "        [13.6687],\n",
      "        [14.0414],\n",
      "        [12.5150],\n",
      "        [14.4537],\n",
      "        [14.2916],\n",
      "        [12.5996],\n",
      "        [12.2318],\n",
      "        [13.6814],\n",
      "        [12.5371],\n",
      "        [16.4129],\n",
      "        [13.9562],\n",
      "        [12.0324],\n",
      "        [14.3626],\n",
      "        [13.7645],\n",
      "        [14.7553],\n",
      "        [13.2679],\n",
      "        [12.6575],\n",
      "        [12.8024],\n",
      "        [12.9377],\n",
      "        [12.7480],\n",
      "        [17.5099],\n",
      "        [15.5534],\n",
      "        [13.4811],\n",
      "        [15.1917],\n",
      "        [14.2946],\n",
      "        [12.6829],\n",
      "        [12.7393],\n",
      "        [12.9284],\n",
      "        [11.7584],\n",
      "        [11.4188],\n",
      "        [11.7393],\n",
      "        [ 8.6382],\n",
      "        [12.1055],\n",
      "        [12.5368],\n",
      "        [14.1324],\n",
      "        [11.9970],\n",
      "        [14.7860],\n",
      "        [15.3272],\n",
      "        [13.6103],\n",
      "        [12.2757],\n",
      "        [13.5000],\n",
      "        [12.0557],\n",
      "        [15.7063],\n",
      "        [14.7009],\n",
      "        [12.6518],\n",
      "        [11.7848],\n",
      "        [12.8217],\n",
      "        [12.4925],\n",
      "        [11.0347],\n",
      "        [11.9538],\n",
      "        [10.5842],\n",
      "        [12.9267],\n",
      "        [13.9270],\n",
      "        [13.8063],\n",
      "        [10.5253],\n",
      "        [13.1583],\n",
      "        [11.4649],\n",
      "        [12.3610],\n",
      "        [13.8390],\n",
      "        [16.7154],\n",
      "        [12.5176],\n",
      "        [15.4786],\n",
      "        [12.2367],\n",
      "        [12.8855],\n",
      "        [13.7315],\n",
      "        [13.9411],\n",
      "        [13.4403],\n",
      "        [14.1860],\n",
      "        [15.0942],\n",
      "        [14.0540],\n",
      "        [15.0574],\n",
      "        [11.0376],\n",
      "        [12.6132],\n",
      "        [ 9.6867],\n",
      "        [10.8885],\n",
      "        [13.8220],\n",
      "        [11.5889],\n",
      "        [12.4777],\n",
      "        [13.4715],\n",
      "        [12.1027],\n",
      "        [15.0795],\n",
      "        [10.8646],\n",
      "        [14.1294],\n",
      "        [11.6369],\n",
      "        [12.1714],\n",
      "        [11.4223],\n",
      "        [ 9.0179],\n",
      "        [10.1371],\n",
      "        [11.7474],\n",
      "        [11.8147],\n",
      "        [13.7510],\n",
      "        [14.8181],\n",
      "        [16.7095],\n",
      "        [14.4952],\n",
      "        [11.4004],\n",
      "        [15.8504],\n",
      "        [11.8615],\n",
      "        [12.9371],\n",
      "        [14.7437],\n",
      "        [12.2091],\n",
      "        [11.9840],\n",
      "        [11.4017],\n",
      "        [10.8699],\n",
      "        [14.1034],\n",
      "        [15.7608],\n",
      "        [13.8431],\n",
      "        [12.0431],\n",
      "        [12.0130],\n",
      "        [10.9862],\n",
      "        [ 9.7718],\n",
      "        [13.9715],\n",
      "        [11.2069],\n",
      "        [12.2457],\n",
      "        [12.9208],\n",
      "        [11.5136],\n",
      "        [12.7592],\n",
      "        [11.0832],\n",
      "        [11.9896],\n",
      "        [11.9765],\n",
      "        [12.0775],\n",
      "        [10.6686],\n",
      "        [11.0847],\n",
      "        [12.9046],\n",
      "        [10.7050],\n",
      "        [10.1366],\n",
      "        [11.5289],\n",
      "        [12.1625],\n",
      "        [14.0736],\n",
      "        [12.2616],\n",
      "        [12.2310],\n",
      "        [12.9707],\n",
      "        [14.7920],\n",
      "        [12.4347],\n",
      "        [13.7881],\n",
      "        [14.7217],\n",
      "        [13.1956],\n",
      "        [15.6049],\n",
      "        [14.8056],\n",
      "        [17.0961],\n",
      "        [16.7292],\n",
      "        [12.8914],\n",
      "        [11.6851],\n",
      "        [12.5687],\n",
      "        [12.7329],\n",
      "        [11.0863],\n",
      "        [10.6662],\n",
      "        [12.7549],\n",
      "        [12.4613],\n",
      "        [13.8011],\n",
      "        [12.3224],\n",
      "        [12.0900],\n",
      "        [12.4229],\n",
      "        [15.2421],\n",
      "        [16.6913],\n",
      "        [11.3739],\n",
      "        [14.8403],\n",
      "        [15.4764],\n",
      "        [16.8539],\n",
      "        [12.6416],\n",
      "        [13.0424],\n",
      "        [11.6226],\n",
      "        [14.9254],\n",
      "        [13.1484],\n",
      "        [12.2709],\n",
      "        [12.7664],\n",
      "        [13.3499],\n",
      "        [13.2103],\n",
      "        [12.5738],\n",
      "        [11.9150],\n",
      "        [14.1494],\n",
      "        [12.9650],\n",
      "        [12.7356],\n",
      "        [12.0318],\n",
      "        [15.8322],\n",
      "        [11.1187],\n",
      "        [14.0775],\n",
      "        [10.3773],\n",
      "        [11.9819],\n",
      "        [11.9718],\n",
      "        [13.3092],\n",
      "        [12.3560],\n",
      "        [10.3895],\n",
      "        [11.5329],\n",
      "        [12.2838],\n",
      "        [11.9630],\n",
      "        [11.7365],\n",
      "        [13.6475],\n",
      "        [14.0694],\n",
      "        [11.6632],\n",
      "        [12.5076],\n",
      "        [11.8189],\n",
      "        [11.9848],\n",
      "        [12.7022],\n",
      "        [10.5763],\n",
      "        [12.8566],\n",
      "        [13.5991],\n",
      "        [11.2880],\n",
      "        [14.8554],\n",
      "        [10.8324],\n",
      "        [10.7616],\n",
      "        [12.7311],\n",
      "        [12.8854],\n",
      "        [13.8997],\n",
      "        [11.9693],\n",
      "        [10.7468],\n",
      "        [12.5806],\n",
      "        [10.5555],\n",
      "        [11.2832],\n",
      "        [11.7580],\n",
      "        [13.7975],\n",
      "        [15.0007],\n",
      "        [12.0474],\n",
      "        [11.2830],\n",
      "        [12.3941],\n",
      "        [11.8276],\n",
      "        [ 9.4666],\n",
      "        [10.8327],\n",
      "        [11.4973],\n",
      "        [12.9803],\n",
      "        [14.7543],\n",
      "        [13.2436],\n",
      "        [12.1566],\n",
      "        [11.9706],\n",
      "        [13.5143],\n",
      "        [12.7201],\n",
      "        [11.1537],\n",
      "        [14.4135],\n",
      "        [14.3891],\n",
      "        [10.7405],\n",
      "        [13.7072],\n",
      "        [13.5906],\n",
      "        [11.4855],\n",
      "        [12.3088],\n",
      "        [10.1670],\n",
      "        [11.5210],\n",
      "        [10.0657],\n",
      "        [11.5464],\n",
      "        [14.8759],\n",
      "        [10.4883],\n",
      "        [14.5145],\n",
      "        [11.0647],\n",
      "        [10.3405],\n",
      "        [12.3749],\n",
      "        [13.6269],\n",
      "        [12.6306],\n",
      "        [11.3692],\n",
      "        [11.7527],\n",
      "        [10.8668],\n",
      "        [14.1061],\n",
      "        [12.5300],\n",
      "        [11.5335],\n",
      "        [12.4003],\n",
      "        [13.2220],\n",
      "        [11.4924],\n",
      "        [12.1509],\n",
      "        [11.8598],\n",
      "        [14.7927],\n",
      "        [11.8668],\n",
      "        [10.2278],\n",
      "        [ 9.9665],\n",
      "        [10.5121],\n",
      "        [12.0436],\n",
      "        [11.7060],\n",
      "        [11.1881],\n",
      "        [14.3494],\n",
      "        [13.3978],\n",
      "        [11.3155],\n",
      "        [11.9166],\n",
      "        [11.9915],\n",
      "        [10.9655],\n",
      "        [13.1614],\n",
      "        [11.3371],\n",
      "        [10.9828],\n",
      "        [11.9239],\n",
      "        [11.8285],\n",
      "        [11.2904],\n",
      "        [10.1019],\n",
      "        [10.0946],\n",
      "        [11.1213],\n",
      "        [10.8756],\n",
      "        [12.9586],\n",
      "        [12.1124],\n",
      "        [10.0756],\n",
      "        [10.9289],\n",
      "        [10.3968],\n",
      "        [11.2501],\n",
      "        [12.5184],\n",
      "        [11.8188],\n",
      "        [14.4704],\n",
      "        [ 9.2225],\n",
      "        [11.2971],\n",
      "        [10.4008],\n",
      "        [10.9691],\n",
      "        [11.8260],\n",
      "        [ 9.7157],\n",
      "        [13.9314],\n",
      "        [10.3729],\n",
      "        [11.9744],\n",
      "        [11.0642],\n",
      "        [12.0825],\n",
      "        [13.8682],\n",
      "        [11.5334],\n",
      "        [11.0500],\n",
      "        [10.1244],\n",
      "        [ 9.4807],\n",
      "        [11.1566],\n",
      "        [11.1886],\n",
      "        [12.0664],\n",
      "        [ 9.2792],\n",
      "        [10.6159],\n",
      "        [10.3779],\n",
      "        [10.9186],\n",
      "        [ 9.7600],\n",
      "        [10.9439],\n",
      "        [11.8907],\n",
      "        [11.3473],\n",
      "        [10.6746],\n",
      "        [ 9.7092],\n",
      "        [10.1772],\n",
      "        [10.2273],\n",
      "        [ 9.9313],\n",
      "        [ 9.7917],\n",
      "        [11.1391],\n",
      "        [10.1768],\n",
      "        [10.5647],\n",
      "        [11.8861],\n",
      "        [12.2030],\n",
      "        [12.3237],\n",
      "        [11.4936],\n",
      "        [11.1446],\n",
      "        [10.9558],\n",
      "        [12.0123],\n",
      "        [10.3765],\n",
      "        [12.1050],\n",
      "        [10.9012],\n",
      "        [10.2379],\n",
      "        [10.3178],\n",
      "        [ 9.6322],\n",
      "        [10.9167],\n",
      "        [12.0143],\n",
      "        [11.3814],\n",
      "        [11.2832],\n",
      "        [12.1913],\n",
      "        [12.8538],\n",
      "        [12.2928],\n",
      "        [11.6038],\n",
      "        [11.2996],\n",
      "        [11.8651],\n",
      "        [11.6495],\n",
      "        [12.0822],\n",
      "        [13.2648],\n",
      "        [10.9274],\n",
      "        [10.3403],\n",
      "        [10.1449],\n",
      "        [10.3833],\n",
      "        [11.8580],\n",
      "        [10.3832],\n",
      "        [11.3616],\n",
      "        [10.1917],\n",
      "        [ 9.6777],\n",
      "        [ 9.6289],\n",
      "        [ 9.9397],\n",
      "        [10.3175],\n",
      "        [10.0636],\n",
      "        [11.9214],\n",
      "        [10.3795],\n",
      "        [11.4890],\n",
      "        [12.4702],\n",
      "        [10.5736],\n",
      "        [ 9.7896],\n",
      "        [ 9.6163],\n",
      "        [11.7362],\n",
      "        [12.1268],\n",
      "        [10.7849],\n",
      "        [11.7551],\n",
      "        [10.3906],\n",
      "        [ 9.4648],\n",
      "        [11.3204],\n",
      "        [ 9.8789],\n",
      "        [11.5175],\n",
      "        [11.0558],\n",
      "        [11.1997],\n",
      "        [11.8403],\n",
      "        [11.1753],\n",
      "        [ 9.8417],\n",
      "        [11.0426],\n",
      "        [10.8697],\n",
      "        [10.6597],\n",
      "        [ 9.9358],\n",
      "        [10.6838],\n",
      "        [12.7688],\n",
      "        [11.9933],\n",
      "        [10.8310],\n",
      "        [10.1528],\n",
      "        [ 8.6213],\n",
      "        [10.3405],\n",
      "        [10.2825],\n",
      "        [10.9071],\n",
      "        [11.0961],\n",
      "        [10.8000],\n",
      "        [ 9.1107],\n",
      "        [11.1189],\n",
      "        [10.6315],\n",
      "        [ 9.8803],\n",
      "        [ 9.6437],\n",
      "        [ 9.9269],\n",
      "        [ 9.7444],\n",
      "        [10.6807],\n",
      "        [ 9.4685],\n",
      "        [10.6056],\n",
      "        [10.4908],\n",
      "        [ 9.5228],\n",
      "        [10.0250],\n",
      "        [ 8.8797],\n",
      "        [10.5195],\n",
      "        [ 9.8061],\n",
      "        [10.3129],\n",
      "        [10.1255],\n",
      "        [10.7357],\n",
      "        [10.4484],\n",
      "        [10.4567],\n",
      "        [ 9.3047],\n",
      "        [10.4922],\n",
      "        [12.0218],\n",
      "        [10.4612],\n",
      "        [ 9.8574],\n",
      "        [ 8.8754],\n",
      "        [ 8.0567],\n",
      "        [10.1272],\n",
      "        [10.3908],\n",
      "        [ 9.7833],\n",
      "        [10.3460],\n",
      "        [ 9.6671],\n",
      "        [10.8287],\n",
      "        [11.4272],\n",
      "        [13.3347],\n",
      "        [10.4088],\n",
      "        [10.7667],\n",
      "        [10.3364],\n",
      "        [10.9335],\n",
      "        [10.2893],\n",
      "        [10.3573],\n",
      "        [10.8706],\n",
      "        [10.3481],\n",
      "        [11.2135],\n",
      "        [10.6272],\n",
      "        [10.4678],\n",
      "        [11.5848],\n",
      "        [12.0703],\n",
      "        [10.1891],\n",
      "        [ 8.6547],\n",
      "        [ 9.5668],\n",
      "        [ 8.6682],\n",
      "        [10.4999],\n",
      "        [ 9.6568],\n",
      "        [ 9.9304],\n",
      "        [10.8883],\n",
      "        [ 9.0592],\n",
      "        [ 8.6050],\n",
      "        [10.8861],\n",
      "        [ 9.0337],\n",
      "        [ 9.4445],\n",
      "        [ 9.2558],\n",
      "        [ 9.0796],\n",
      "        [10.0195],\n",
      "        [10.4599],\n",
      "        [ 9.9643],\n",
      "        [ 9.2228],\n",
      "        [11.6509],\n",
      "        [10.1321],\n",
      "        [ 9.8863],\n",
      "        [ 9.3318],\n",
      "        [10.5479],\n",
      "        [10.0978],\n",
      "        [11.3056],\n",
      "        [ 8.5427],\n",
      "        [ 9.1236],\n",
      "        [ 8.9260],\n",
      "        [10.1078],\n",
      "        [ 8.7163],\n",
      "        [ 9.7636],\n",
      "        [ 9.8109],\n",
      "        [ 9.1873],\n",
      "        [ 9.5019],\n",
      "        [ 9.2226],\n",
      "        [ 9.1117],\n",
      "        [ 8.8716],\n",
      "        [ 8.8532],\n",
      "        [ 9.2576],\n",
      "        [ 9.9090],\n",
      "        [ 9.8381],\n",
      "        [10.5286],\n",
      "        [ 9.7450],\n",
      "        [ 9.0116],\n",
      "        [ 9.7070],\n",
      "        [ 9.9067],\n",
      "        [ 9.8511],\n",
      "        [10.9560],\n",
      "        [10.9512],\n",
      "        [ 9.8133],\n",
      "        [10.9033],\n",
      "        [10.8922],\n",
      "        [11.8517],\n",
      "        [ 9.7986],\n",
      "        [ 9.2199],\n",
      "        [ 9.6189],\n",
      "        [ 9.2119],\n",
      "        [ 8.7699],\n",
      "        [ 8.2259],\n",
      "        [ 9.6872],\n",
      "        [ 9.7992],\n",
      "        [10.7209],\n",
      "        [ 9.7184],\n",
      "        [ 9.5181],\n",
      "        [ 8.7497],\n",
      "        [ 8.9671],\n",
      "        [ 8.1885],\n",
      "        [ 8.0748],\n",
      "        [ 8.8397],\n",
      "        [ 9.7246],\n",
      "        [ 9.7375],\n",
      "        [ 8.8637],\n",
      "        [10.1991],\n",
      "        [10.1599],\n",
      "        [10.6658],\n",
      "        [10.0111],\n",
      "        [ 9.4788],\n",
      "        [ 9.0069],\n",
      "        [ 9.5140],\n",
      "        [ 9.2239],\n",
      "        [12.0908],\n",
      "        [ 9.9626],\n",
      "        [ 9.2072],\n",
      "        [ 9.7610],\n",
      "        [ 9.5445],\n",
      "        [10.0194],\n",
      "        [ 8.6055],\n",
      "        [ 9.2453],\n",
      "        [ 8.3433],\n",
      "        [ 9.2666],\n",
      "        [ 9.8429],\n",
      "        [ 8.9814],\n",
      "        [ 8.7635],\n",
      "        [ 9.3040],\n",
      "        [ 9.5468],\n",
      "        [ 9.2786],\n",
      "        [ 9.4997],\n",
      "        [ 9.4275],\n",
      "        [ 9.9498],\n",
      "        [ 8.8605],\n",
      "        [ 9.4582],\n",
      "        [ 8.5762],\n",
      "        [ 7.5818],\n",
      "        [ 8.9270],\n",
      "        [ 8.1502],\n",
      "        [10.3594],\n",
      "        [ 9.6497],\n",
      "        [ 8.6950],\n",
      "        [10.3907],\n",
      "        [ 9.3355],\n",
      "        [ 9.1542],\n",
      "        [10.3872],\n",
      "        [ 8.2113],\n",
      "        [ 8.1142],\n",
      "        [ 8.5635],\n",
      "        [ 9.4438],\n",
      "        [ 9.8602],\n",
      "        [ 8.6593],\n",
      "        [ 8.1861],\n",
      "        [ 8.9235],\n",
      "        [ 9.7360],\n",
      "        [ 8.8454],\n",
      "        [ 9.2146],\n",
      "        [10.7338],\n",
      "        [ 9.7077],\n",
      "        [10.3411],\n",
      "        [ 8.8568],\n",
      "        [ 8.5564],\n",
      "        [ 7.6563],\n",
      "        [ 8.9636],\n",
      "        [ 8.3408],\n",
      "        [ 8.6707],\n",
      "        [ 8.3495],\n",
      "        [ 7.8517],\n",
      "        [ 8.0112],\n",
      "        [ 7.9948],\n",
      "        [ 8.2115],\n",
      "        [ 8.0173],\n",
      "        [ 9.1656],\n",
      "        [ 9.4504],\n",
      "        [ 7.9916],\n",
      "        [ 9.2119],\n",
      "        [ 8.8465],\n",
      "        [ 8.4453],\n",
      "        [ 8.2906],\n",
      "        [ 9.4089],\n",
      "        [ 9.0867],\n",
      "        [ 8.6201],\n",
      "        [ 9.0528],\n",
      "        [ 8.0625],\n",
      "        [ 8.4452],\n",
      "        [ 8.6161],\n",
      "        [ 8.3515],\n",
      "        [ 8.6959],\n",
      "        [ 9.3224],\n",
      "        [ 8.3535],\n",
      "        [ 8.8590],\n",
      "        [ 8.8858],\n",
      "        [ 7.6998],\n",
      "        [ 8.3457],\n",
      "        [ 7.8303],\n",
      "        [ 7.6798],\n",
      "        [ 8.3855],\n",
      "        [ 8.3233],\n",
      "        [ 8.2173],\n",
      "        [ 8.3566],\n",
      "        [ 8.2764],\n",
      "        [ 8.5404],\n",
      "        [ 9.4469],\n",
      "        [ 7.8215],\n",
      "        [ 8.2138],\n",
      "        [ 7.5096],\n",
      "        [ 8.5412],\n",
      "        [ 7.8986],\n",
      "        [ 8.4911],\n",
      "        [ 8.1205],\n",
      "        [ 8.2393],\n",
      "        [ 8.3696],\n",
      "        [ 8.7077],\n",
      "        [ 8.0741],\n",
      "        [ 7.5947],\n",
      "        [ 8.2654],\n",
      "        [ 8.5677],\n",
      "        [ 8.6361],\n",
      "        [ 8.2771],\n",
      "        [ 7.2367],\n",
      "        [ 7.2085],\n",
      "        [ 7.8434],\n",
      "        [ 8.1475],\n",
      "        [ 7.8424],\n",
      "        [ 8.3940],\n",
      "        [ 8.0827],\n",
      "        [ 8.4546],\n",
      "        [ 8.5157],\n",
      "        [ 7.9919],\n",
      "        [ 7.9187],\n",
      "        [ 9.1519],\n",
      "        [ 7.4925],\n",
      "        [ 7.5443],\n",
      "        [ 7.9548],\n",
      "        [ 7.5452],\n",
      "        [ 7.8130],\n",
      "        [ 8.5389],\n",
      "        [ 9.1635],\n",
      "        [ 7.8438],\n",
      "        [ 7.4370],\n",
      "        [ 8.0653],\n",
      "        [ 7.5823],\n",
      "        [ 7.9818],\n",
      "        [ 7.9501],\n",
      "        [ 8.5692],\n",
      "        [ 8.1646],\n",
      "        [ 8.3529],\n",
      "        [ 8.5363],\n",
      "        [ 8.4839],\n",
      "        [ 7.4170],\n",
      "        [ 8.0122],\n",
      "        [ 9.3290],\n",
      "        [ 7.2822],\n",
      "        [ 7.0438],\n",
      "        [ 8.2806],\n",
      "        [ 7.5165],\n",
      "        [ 7.0627],\n",
      "        [ 9.4547],\n",
      "        [ 7.5568],\n",
      "        [ 7.6722],\n",
      "        [ 7.0894],\n",
      "        [ 7.3855],\n",
      "        [ 7.4123],\n",
      "        [ 7.5375],\n",
      "        [ 7.3946],\n",
      "        [ 8.1255],\n",
      "        [ 6.6632],\n",
      "        [ 7.6848],\n",
      "        [ 7.1830],\n",
      "        [ 7.1994],\n",
      "        [ 6.5709],\n",
      "        [ 7.4940],\n",
      "        [ 8.0168],\n",
      "        [ 7.5911],\n",
      "        [ 6.9696],\n",
      "        [ 7.2271],\n",
      "        [ 6.9485],\n",
      "        [ 7.6892],\n",
      "        [ 7.6481],\n",
      "        [ 7.3827],\n",
      "        [ 6.6025],\n",
      "        [ 7.2285],\n",
      "        [ 6.3884],\n",
      "        [ 6.2892],\n",
      "        [ 7.2744],\n",
      "        [ 7.8047],\n",
      "        [ 7.6445],\n",
      "        [ 7.3775],\n",
      "        [ 6.8720],\n",
      "        [ 6.5966],\n",
      "        [ 6.6120],\n",
      "        [ 7.1426],\n",
      "        [ 7.8302],\n",
      "        [ 7.5777],\n",
      "        [ 7.2979],\n",
      "        [ 6.5765],\n",
      "        [ 6.6078],\n",
      "        [ 6.3278],\n",
      "        [ 7.1799],\n",
      "        [ 7.0174],\n",
      "        [ 6.8586],\n",
      "        [ 6.7901],\n",
      "        [ 6.9787],\n",
      "        [ 6.6094],\n",
      "        [ 6.3121],\n",
      "        [ 6.6827],\n",
      "        [ 7.5762],\n",
      "        [ 6.4481],\n",
      "        [ 6.6232],\n",
      "        [ 6.8820],\n",
      "        [ 5.7165],\n",
      "        [ 6.7180],\n",
      "        [ 6.7877],\n",
      "        [ 7.4837],\n",
      "        [ 6.8005],\n",
      "        [ 6.2603],\n",
      "        [ 6.1250],\n",
      "        [ 6.0532],\n",
      "        [ 6.6500],\n",
      "        [ 6.3980],\n",
      "        [ 6.8018],\n",
      "        [ 6.4488],\n",
      "        [ 6.1364],\n",
      "        [ 5.9379],\n",
      "        [ 6.3951],\n",
      "        [ 6.1525],\n",
      "        [ 6.4517],\n",
      "        [ 6.6549],\n",
      "        [ 6.3023],\n",
      "        [ 6.4452],\n",
      "        [ 6.1484],\n",
      "        [ 6.5629],\n",
      "        [ 5.8606],\n",
      "        [ 5.9714],\n",
      "        [ 6.1663],\n",
      "        [ 6.0691],\n",
      "        [ 6.1089],\n",
      "        [ 6.1392],\n",
      "        [ 6.1501],\n",
      "        [ 6.1164],\n",
      "        [ 6.6069],\n",
      "        [ 5.2188],\n",
      "        [ 5.6658],\n",
      "        [ 6.0274],\n",
      "        [ 5.8250],\n",
      "        [ 5.8742],\n",
      "        [ 5.6796],\n",
      "        [ 5.8115],\n",
      "        [ 5.5243],\n",
      "        [ 5.4368],\n",
      "        [ 5.7466],\n",
      "        [ 6.0756],\n",
      "        [ 6.0007],\n",
      "        [ 5.3701],\n",
      "        [ 5.2319],\n",
      "        [ 5.4778],\n",
      "        [ 5.5640],\n",
      "        [ 5.4681],\n",
      "        [ 5.3404],\n",
      "        [ 5.3548],\n",
      "        [ 5.5681],\n",
      "        [ 5.0028],\n",
      "        [ 4.7931],\n",
      "        [ 4.7814],\n",
      "        [ 5.1290],\n",
      "        [ 5.0004],\n",
      "        [ 4.8658],\n",
      "        [ 4.3084],\n",
      "        [ 5.3191],\n",
      "        [ 4.7647],\n",
      "        [ 5.0156],\n",
      "        [ 4.1354],\n",
      "        [ 4.1569],\n",
      "        [ 4.8568],\n",
      "        [ 4.8048],\n",
      "        [ 4.6492],\n",
      "        [ 5.0096],\n",
      "        [ 4.2105],\n",
      "        [ 4.5991],\n",
      "        [ 4.5441],\n",
      "        [ 4.2484],\n",
      "        [ 4.1671],\n",
      "        [ 4.2417],\n",
      "        [ 4.0875],\n",
      "        [ 4.1405],\n",
      "        [ 4.4069],\n",
      "        [ 4.2451],\n",
      "        [ 4.0757],\n",
      "        [ 3.7402],\n",
      "        [ 3.3652],\n",
      "        [ 4.0385],\n",
      "        [ 3.8263],\n",
      "        [ 3.3782],\n",
      "        [ 5.2407],\n",
      "        [ 2.9431],\n",
      "        [ 2.6623],\n",
      "        [ 3.0533],\n",
      "        [ 2.7404],\n",
      "        [ 3.2178],\n",
      "        [ 3.5464],\n",
      "        [ 2.6575],\n",
      "        [ 2.5563],\n",
      "        [ 2.3202],\n",
      "        [ 1.2535]])\n"
     ]
    }
   ],
   "source": [
    "def forward(SamplesAll, wOpt):\n",
    "    \"\"\"\n",
    "    Get model output\n",
    "    param: SamplesAll, a matrix with dimensions NumSamples x NumDimensions \n",
    "    return: Estimates the model outputs activation function \n",
    "    \"\"\"\n",
    "    SamplesAll = SamplesAll.to(torch.float32)\n",
    "    wOpt = wOpt.to(torch.float32)\n",
    "    \n",
    "    EstimatedTargets = SamplesAll.mm(wOpt)\n",
    "    \n",
    "    return EstimatedTargets\n",
    "\n",
    "# Estimated target for sample data    \n",
    "estimatedTargetsAll = forward(SamplesAll, wOpt)\n",
    "\n",
    "print(estimatedTargetsAll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Calcule la p茅rdida utilizando el error cuadr谩tico medio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 82.901, RMSE: 9.105\n"
     ]
    }
   ],
   "source": [
    "# 4) Se eval煤a el error\n",
    "def evaluateError(TargetsAll, EstimatedTargetsAll):\n",
    "    \"\"\"\n",
    "    Evaluate model error using the euclidian distance.\n",
    "    param TargetsAll, real targets\n",
    "    param EstimatedTargets\n",
    "    \"\"\"\n",
    "    error = torch.norm(TargetsAll - EstimatedTargetsAll, 2)\n",
    "    return error\n",
    "  \n",
    "  \n",
    "# Error for sample data  \n",
    "mse = evaluateError(targetsAll, estimatedTargetsAll)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "Realice al menos tres conclusiones sobre el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El error cuadr谩tico medio de una aproximaci贸n por medio de perceptrones es por lo general menos que la aproximaci贸n por medio de perceptrones multicapa ya que se pasa por menos an谩lisis.\n",
    "- A pesar de tener un error cuadr谩tico medio mayor note que la diferencia con respecto a las aproximaciones del ejercicio anterior no son tan lejanas, por lo que en la ausencia de recursos se podr铆a usar esata aproximaci贸n sin problemas.\n",
    "- El ajuste a la ecuaci贸n de mejor ajuste para los abalones permite obtener de forma lineal una forma de predecir el comportamiento de la cantidad de anillos que poseeera el abalon y por lo tanto facilita la investigaci贸n al hacerla mucho m谩s r谩pido con predicciones confiables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 5. Implemente un perceptr贸n de una capa \n",
    "\n",
    "(requisito indispensable para tomar en cuenta el ejercicio, deben usar tensores de PyTorch).\n",
    "\n",
    "- a)(10 puntos) Implemente el algoritmo del perceptr贸n de una capa rescindiendo al m谩ximo de estructuras de tipo for, usando en su lugar operaciones matriciales. Debe implementarlo sin utilizar ninguna biblioteca, es decir en PyTorch no se puede usar ninguna clase o funcionalidad desarrollada por PyTorch o alguna otra biblioteca.\n",
    "- b)(5 puntos) Utilice el perceptr贸n desarrollado en a) para realizar regresi贸n con los datos de los abulones.\n",
    "- c) (3 puntos) Realice al menos tres conclusiones sobre el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "Implemente el algoritmo del perceptr贸n de una capa rescindiendo al m谩ximo de estructuras de tipo for, usando en su lugar operaciones matriciales.\n",
    "\n",
    "### b)\n",
    "\n",
    "Utilice el perceptr贸n desarrollado en a) para realizar regresi贸n con los datos de los abulones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18.4987],\n",
      "        [21.6167],\n",
      "        [19.9886],\n",
      "        [20.9833],\n",
      "        [21.5479],\n",
      "        [17.9752],\n",
      "        [20.7373],\n",
      "        [20.0601],\n",
      "        [21.5280],\n",
      "        [16.3790],\n",
      "        [20.9782],\n",
      "        [19.6770],\n",
      "        [18.6613],\n",
      "        [19.0843],\n",
      "        [21.3992],\n",
      "        [19.0787],\n",
      "        [18.5971],\n",
      "        [18.1823],\n",
      "        [18.0720],\n",
      "        [20.5646],\n",
      "        [18.9642],\n",
      "        [17.9968],\n",
      "        [18.3175],\n",
      "        [21.2679],\n",
      "        [18.1676],\n",
      "        [16.8948],\n",
      "        [17.9552],\n",
      "        [19.0630],\n",
      "        [18.0787],\n",
      "        [19.2560],\n",
      "        [18.3127],\n",
      "        [17.5240],\n",
      "        [17.5016],\n",
      "        [18.2788],\n",
      "        [18.3463],\n",
      "        [18.6576],\n",
      "        [19.5764],\n",
      "        [15.9948],\n",
      "        [18.7723],\n",
      "        [19.9341],\n",
      "        [21.0300],\n",
      "        [17.4987],\n",
      "        [19.1356],\n",
      "        [17.7975],\n",
      "        [17.0930],\n",
      "        [17.5989],\n",
      "        [20.1104],\n",
      "        [18.3617],\n",
      "        [18.3986],\n",
      "        [17.9854],\n",
      "        [19.0710],\n",
      "        [17.1742],\n",
      "        [17.0978],\n",
      "        [17.8662],\n",
      "        [17.7200],\n",
      "        [16.5259],\n",
      "        [15.5943],\n",
      "        [18.1219],\n",
      "        [16.5819],\n",
      "        [19.5046],\n",
      "        [16.7514],\n",
      "        [17.2326],\n",
      "        [17.7437],\n",
      "        [15.4684],\n",
      "        [16.0458],\n",
      "        [16.5450],\n",
      "        [17.3998],\n",
      "        [18.3751],\n",
      "        [15.0744],\n",
      "        [17.1898],\n",
      "        [16.0382],\n",
      "        [17.2836],\n",
      "        [18.6550],\n",
      "        [16.9705],\n",
      "        [17.3841],\n",
      "        [14.5999],\n",
      "        [16.5644],\n",
      "        [16.6288],\n",
      "        [16.1346],\n",
      "        [17.8689],\n",
      "        [14.9778],\n",
      "        [16.8391],\n",
      "        [16.7968],\n",
      "        [17.0934],\n",
      "        [17.4915],\n",
      "        [17.3403],\n",
      "        [18.1297],\n",
      "        [16.8608],\n",
      "        [16.6356],\n",
      "        [15.0729],\n",
      "        [17.0614],\n",
      "        [15.9473],\n",
      "        [16.5479],\n",
      "        [18.9209],\n",
      "        [17.5149],\n",
      "        [16.7781],\n",
      "        [16.0868],\n",
      "        [15.6068],\n",
      "        [15.7836],\n",
      "        [15.4095],\n",
      "        [14.7573],\n",
      "        [16.3307],\n",
      "        [16.0918],\n",
      "        [17.4952],\n",
      "        [17.1497],\n",
      "        [17.1215],\n",
      "        [17.5929],\n",
      "        [16.7684],\n",
      "        [15.3090],\n",
      "        [15.3852],\n",
      "        [15.7365],\n",
      "        [15.8394],\n",
      "        [16.1672],\n",
      "        [15.5126],\n",
      "        [14.5373],\n",
      "        [15.5969],\n",
      "        [15.8980],\n",
      "        [14.3754],\n",
      "        [15.7009],\n",
      "        [15.8792],\n",
      "        [15.9323],\n",
      "        [15.3530],\n",
      "        [15.2692],\n",
      "        [16.6713],\n",
      "        [16.2499],\n",
      "        [16.6748],\n",
      "        [15.8191],\n",
      "        [14.7039],\n",
      "        [14.1593],\n",
      "        [15.2729],\n",
      "        [15.8217],\n",
      "        [18.0602],\n",
      "        [17.3324],\n",
      "        [15.6742],\n",
      "        [14.4909],\n",
      "        [13.9831],\n",
      "        [16.5566],\n",
      "        [15.4269],\n",
      "        [15.6311],\n",
      "        [14.9895],\n",
      "        [14.7004],\n",
      "        [14.7343],\n",
      "        [16.1297],\n",
      "        [15.4568],\n",
      "        [13.9356],\n",
      "        [17.2220],\n",
      "        [15.5152],\n",
      "        [16.8875],\n",
      "        [17.3144],\n",
      "        [16.6229],\n",
      "        [14.0649],\n",
      "        [15.8819],\n",
      "        [16.9530],\n",
      "        [14.7517],\n",
      "        [17.2625],\n",
      "        [15.3330],\n",
      "        [16.2577],\n",
      "        [15.2433],\n",
      "        [15.1788],\n",
      "        [14.1904],\n",
      "        [13.8943],\n",
      "        [15.1162],\n",
      "        [15.9525],\n",
      "        [15.2709],\n",
      "        [16.5527],\n",
      "        [17.0385],\n",
      "        [14.7807],\n",
      "        [13.6307],\n",
      "        [15.5984],\n",
      "        [14.5073],\n",
      "        [15.4055],\n",
      "        [13.6039],\n",
      "        [14.1823],\n",
      "        [15.1178],\n",
      "        [15.1774],\n",
      "        [14.9198],\n",
      "        [15.3077],\n",
      "        [15.8013],\n",
      "        [14.5851],\n",
      "        [14.3859],\n",
      "        [14.0603],\n",
      "        [13.2794],\n",
      "        [14.9391],\n",
      "        [14.2778],\n",
      "        [13.8521],\n",
      "        [14.3311],\n",
      "        [13.6129],\n",
      "        [15.4295],\n",
      "        [14.3277],\n",
      "        [14.2683],\n",
      "        [15.5093],\n",
      "        [14.7117],\n",
      "        [14.7683],\n",
      "        [14.6117],\n",
      "        [13.8480],\n",
      "        [14.5179],\n",
      "        [14.0134],\n",
      "        [15.2158],\n",
      "        [13.1348],\n",
      "        [13.8948],\n",
      "        [13.2703],\n",
      "        [14.6870],\n",
      "        [14.1625],\n",
      "        [14.8279],\n",
      "        [14.0923],\n",
      "        [13.5565],\n",
      "        [13.0545],\n",
      "        [13.3914],\n",
      "        [17.2231],\n",
      "        [15.4865],\n",
      "        [15.1857],\n",
      "        [14.9635],\n",
      "        [14.2724],\n",
      "        [14.9676],\n",
      "        [15.0023],\n",
      "        [12.9047],\n",
      "        [13.2546],\n",
      "        [13.9970],\n",
      "        [14.4426],\n",
      "        [14.8701],\n",
      "        [15.5681],\n",
      "        [15.2560],\n",
      "        [15.2101],\n",
      "        [14.8108],\n",
      "        [12.8940],\n",
      "        [13.8436],\n",
      "        [13.4856],\n",
      "        [13.7597],\n",
      "        [13.7138],\n",
      "        [15.3414],\n",
      "        [14.6926],\n",
      "        [16.1892],\n",
      "        [16.8171],\n",
      "        [14.1248],\n",
      "        [12.9242],\n",
      "        [13.6140],\n",
      "        [13.6500],\n",
      "        [12.6490],\n",
      "        [13.8389],\n",
      "        [12.8348],\n",
      "        [14.6128],\n",
      "        [12.1608],\n",
      "        [14.1630],\n",
      "        [13.8241],\n",
      "        [14.7728],\n",
      "        [14.0676],\n",
      "        [13.8519],\n",
      "        [13.8761],\n",
      "        [11.9604],\n",
      "        [15.5156],\n",
      "        [15.4883],\n",
      "        [14.9058],\n",
      "        [14.2100],\n",
      "        [13.3226],\n",
      "        [13.7042],\n",
      "        [13.7410],\n",
      "        [12.4884],\n",
      "        [12.9599],\n",
      "        [13.7393],\n",
      "        [13.7830],\n",
      "        [13.7241],\n",
      "        [16.9538],\n",
      "        [14.6568],\n",
      "        [14.6107],\n",
      "        [14.5770],\n",
      "        [13.6156],\n",
      "        [14.2450],\n",
      "        [14.0982],\n",
      "        [12.7517],\n",
      "        [13.8321],\n",
      "        [12.4743],\n",
      "        [13.7420],\n",
      "        [14.5182],\n",
      "        [11.3349],\n",
      "        [13.5385],\n",
      "        [13.2179],\n",
      "        [13.2596],\n",
      "        [12.7156],\n",
      "        [14.6667],\n",
      "        [13.4078],\n",
      "        [14.6939],\n",
      "        [15.0917],\n",
      "        [13.6805],\n",
      "        [13.6536],\n",
      "        [12.2921],\n",
      "        [13.5588],\n",
      "        [13.3478],\n",
      "        [12.6574],\n",
      "        [13.2533],\n",
      "        [14.5466],\n",
      "        [13.8168],\n",
      "        [13.9101],\n",
      "        [12.2719],\n",
      "        [12.4469],\n",
      "        [12.2363],\n",
      "        [12.7419],\n",
      "        [13.7729],\n",
      "        [13.5779],\n",
      "        [13.2210],\n",
      "        [14.0740],\n",
      "        [14.5839],\n",
      "        [15.2979],\n",
      "        [13.3010],\n",
      "        [13.2534],\n",
      "        [12.0631],\n",
      "        [12.8621],\n",
      "        [12.8225],\n",
      "        [12.1393],\n",
      "        [12.2661],\n",
      "        [11.6742],\n",
      "        [13.6701],\n",
      "        [12.4350],\n",
      "        [12.5871],\n",
      "        [11.8920],\n",
      "        [13.9292],\n",
      "        [13.8132],\n",
      "        [12.6343],\n",
      "        [14.3710],\n",
      "        [13.1589],\n",
      "        [14.0663],\n",
      "        [13.1170],\n",
      "        [13.4524],\n",
      "        [12.5235],\n",
      "        [12.7461],\n",
      "        [11.2672],\n",
      "        [13.6123],\n",
      "        [12.6992],\n",
      "        [12.5597],\n",
      "        [11.1948],\n",
      "        [11.8474],\n",
      "        [12.2471],\n",
      "        [11.7898],\n",
      "        [12.5734],\n",
      "        [13.8464],\n",
      "        [13.1349],\n",
      "        [16.1225],\n",
      "        [13.5239],\n",
      "        [14.3423],\n",
      "        [13.5133],\n",
      "        [11.7638],\n",
      "        [12.2764],\n",
      "        [11.9335],\n",
      "        [13.7555],\n",
      "        [12.4885],\n",
      "        [12.1240],\n",
      "        [11.3727],\n",
      "        [11.8149],\n",
      "        [12.9061],\n",
      "        [13.1599],\n",
      "        [12.5112],\n",
      "        [12.3501],\n",
      "        [11.6351],\n",
      "        [11.6340],\n",
      "        [13.1139],\n",
      "        [12.6316],\n",
      "        [12.8466],\n",
      "        [12.2788],\n",
      "        [13.1069],\n",
      "        [12.7715],\n",
      "        [11.8917],\n",
      "        [12.4493],\n",
      "        [12.1640],\n",
      "        [11.6299],\n",
      "        [12.9125],\n",
      "        [11.1827],\n",
      "        [12.3938],\n",
      "        [11.4523],\n",
      "        [13.2697],\n",
      "        [11.5595],\n",
      "        [10.5292],\n",
      "        [12.0234],\n",
      "        [12.6085],\n",
      "        [12.1398],\n",
      "        [13.2504],\n",
      "        [11.5679],\n",
      "        [12.0671],\n",
      "        [11.9880],\n",
      "        [11.9272],\n",
      "        [13.1697],\n",
      "        [12.3325],\n",
      "        [13.1412],\n",
      "        [12.5955],\n",
      "        [13.7671],\n",
      "        [14.3924],\n",
      "        [12.2714],\n",
      "        [11.6265],\n",
      "        [11.9049],\n",
      "        [12.6896],\n",
      "        [10.8985],\n",
      "        [10.6399],\n",
      "        [12.7605],\n",
      "        [14.3239],\n",
      "        [12.8275],\n",
      "        [12.2335],\n",
      "        [12.7056],\n",
      "        [11.4644],\n",
      "        [12.3882],\n",
      "        [12.7347],\n",
      "        [11.0340],\n",
      "        [11.8292],\n",
      "        [13.7612],\n",
      "        [12.4066],\n",
      "        [12.7133],\n",
      "        [11.5454],\n",
      "        [11.4111],\n",
      "        [11.2667],\n",
      "        [10.2796],\n",
      "        [10.2399],\n",
      "        [14.0291],\n",
      "        [11.8282],\n",
      "        [11.7640],\n",
      "        [12.9138],\n",
      "        [10.5527],\n",
      "        [10.4128],\n",
      "        [10.8416],\n",
      "        [11.4833],\n",
      "        [11.0637],\n",
      "        [10.9179],\n",
      "        [12.2257],\n",
      "        [10.9433],\n",
      "        [10.9999],\n",
      "        [12.3251],\n",
      "        [10.9852],\n",
      "        [12.1264],\n",
      "        [12.0003],\n",
      "        [12.0398],\n",
      "        [11.1798],\n",
      "        [11.6288],\n",
      "        [13.2288],\n",
      "        [11.7716],\n",
      "        [11.2040],\n",
      "        [10.6018],\n",
      "        [10.8292],\n",
      "        [12.9268],\n",
      "        [10.1852],\n",
      "        [11.4293],\n",
      "        [10.9241],\n",
      "        [12.4570],\n",
      "        [10.9494],\n",
      "        [10.8555],\n",
      "        [11.9757],\n",
      "        [10.8853],\n",
      "        [11.7013],\n",
      "        [10.2896],\n",
      "        [10.8832],\n",
      "        [10.8188],\n",
      "        [10.3779],\n",
      "        [11.3375],\n",
      "        [ 9.8355],\n",
      "        [11.4364],\n",
      "        [11.0766],\n",
      "        [11.8735],\n",
      "        [10.1461],\n",
      "        [10.9304],\n",
      "        [10.3403],\n",
      "        [12.5324],\n",
      "        [11.7190],\n",
      "        [10.9695],\n",
      "        [11.0963],\n",
      "        [10.8557],\n",
      "        [10.9759],\n",
      "        [12.6301],\n",
      "        [12.3840],\n",
      "        [10.6907],\n",
      "        [10.6922],\n",
      "        [10.4267],\n",
      "        [10.0112],\n",
      "        [10.9333],\n",
      "        [11.1142],\n",
      "        [10.8098],\n",
      "        [11.8185],\n",
      "        [12.6313],\n",
      "        [10.8608],\n",
      "        [10.5895],\n",
      "        [ 9.0731],\n",
      "        [10.5616],\n",
      "        [11.4824],\n",
      "        [11.0059],\n",
      "        [11.4179],\n",
      "        [10.2773],\n",
      "        [10.5507],\n",
      "        [10.6443],\n",
      "        [ 9.3328],\n",
      "        [10.8026],\n",
      "        [10.1763],\n",
      "        [10.5817],\n",
      "        [10.1370],\n",
      "        [10.5743],\n",
      "        [ 9.5341],\n",
      "        [10.3034],\n",
      "        [10.5793],\n",
      "        [ 9.7699],\n",
      "        [10.7618],\n",
      "        [10.7935],\n",
      "        [ 9.0087],\n",
      "        [11.2355],\n",
      "        [11.2594],\n",
      "        [10.2621],\n",
      "        [10.2135],\n",
      "        [ 9.6792],\n",
      "        [10.2263],\n",
      "        [ 9.6512],\n",
      "        [10.6948],\n",
      "        [ 9.8044],\n",
      "        [11.2943],\n",
      "        [10.6145],\n",
      "        [ 9.5978],\n",
      "        [10.1380],\n",
      "        [11.0280],\n",
      "        [10.0470],\n",
      "        [11.6110],\n",
      "        [ 9.3092],\n",
      "        [11.2618],\n",
      "        [10.5190],\n",
      "        [10.6174],\n",
      "        [ 9.9494],\n",
      "        [10.2815],\n",
      "        [ 9.3751],\n",
      "        [10.9232],\n",
      "        [10.5887],\n",
      "        [10.6934],\n",
      "        [ 9.8700],\n",
      "        [ 9.8145],\n",
      "        [ 9.8066],\n",
      "        [ 8.9538],\n",
      "        [10.6087],\n",
      "        [10.2831],\n",
      "        [ 9.9844],\n",
      "        [ 9.5923],\n",
      "        [ 9.6038],\n",
      "        [ 9.8028],\n",
      "        [ 8.9356],\n",
      "        [ 8.8419],\n",
      "        [ 9.3689],\n",
      "        [11.0135],\n",
      "        [ 9.8032],\n",
      "        [ 9.3015],\n",
      "        [11.5661],\n",
      "        [10.0998],\n",
      "        [10.1708],\n",
      "        [10.3283],\n",
      "        [ 8.4162],\n",
      "        [ 9.2931],\n",
      "        [ 8.8149],\n",
      "        [ 9.5206],\n",
      "        [ 9.4794],\n",
      "        [ 9.4382],\n",
      "        [ 9.8554],\n",
      "        [ 8.6595],\n",
      "        [ 9.0078],\n",
      "        [ 8.6015],\n",
      "        [10.2908],\n",
      "        [ 9.5354],\n",
      "        [ 9.4146],\n",
      "        [ 9.4432],\n",
      "        [10.1359],\n",
      "        [ 8.9000],\n",
      "        [ 9.8958],\n",
      "        [ 9.4017],\n",
      "        [ 9.4817],\n",
      "        [ 8.4723],\n",
      "        [ 9.2424],\n",
      "        [ 9.4623],\n",
      "        [10.5083],\n",
      "        [ 8.9764],\n",
      "        [ 9.7220],\n",
      "        [ 9.7861],\n",
      "        [10.1251],\n",
      "        [ 8.9389],\n",
      "        [ 9.2488],\n",
      "        [ 9.5963],\n",
      "        [ 8.9501],\n",
      "        [ 8.7022],\n",
      "        [ 9.2802],\n",
      "        [ 8.7828],\n",
      "        [ 9.7735],\n",
      "        [ 9.6634],\n",
      "        [ 9.5551],\n",
      "        [ 9.8459],\n",
      "        [ 9.3863],\n",
      "        [ 8.7505],\n",
      "        [ 8.6757],\n",
      "        [ 8.9408],\n",
      "        [ 8.3308],\n",
      "        [ 8.7874],\n",
      "        [ 8.9058],\n",
      "        [ 8.8369],\n",
      "        [ 9.9219],\n",
      "        [ 8.5512],\n",
      "        [ 9.1191],\n",
      "        [ 8.3093],\n",
      "        [ 8.6716],\n",
      "        [ 8.8583],\n",
      "        [ 9.2156],\n",
      "        [ 9.0446],\n",
      "        [10.1875],\n",
      "        [11.5368],\n",
      "        [ 9.1302],\n",
      "        [ 8.6731],\n",
      "        [ 8.9999],\n",
      "        [ 8.8238],\n",
      "        [ 8.8783],\n",
      "        [ 9.1701],\n",
      "        [ 8.4079],\n",
      "        [ 8.3504],\n",
      "        [ 8.8725],\n",
      "        [ 8.6378],\n",
      "        [ 8.8974],\n",
      "        [ 9.1471],\n",
      "        [ 9.0728],\n",
      "        [ 8.8686],\n",
      "        [ 9.2171],\n",
      "        [ 8.8966],\n",
      "        [ 7.6095],\n",
      "        [ 8.9610],\n",
      "        [ 8.0403],\n",
      "        [ 8.2435],\n",
      "        [ 8.2567],\n",
      "        [ 8.6628],\n",
      "        [ 9.0121],\n",
      "        [ 8.3405],\n",
      "        [ 7.8658],\n",
      "        [ 8.7433],\n",
      "        [ 8.7452],\n",
      "        [ 8.3521],\n",
      "        [ 8.5350],\n",
      "        [ 9.1894],\n",
      "        [ 7.8067],\n",
      "        [ 8.2490],\n",
      "        [ 9.4624],\n",
      "        [12.0427],\n",
      "        [ 7.8316],\n",
      "        [ 9.3235],\n",
      "        [ 8.9109],\n",
      "        [ 8.3518],\n",
      "        [ 8.3211],\n",
      "        [ 8.5311],\n",
      "        [ 8.3975],\n",
      "        [ 8.4885],\n",
      "        [ 9.0921],\n",
      "        [ 7.6961],\n",
      "        [ 7.5672],\n",
      "        [ 8.2448],\n",
      "        [ 7.6900],\n",
      "        [ 8.2784],\n",
      "        [ 7.6019],\n",
      "        [ 8.2271],\n",
      "        [ 8.2471],\n",
      "        [ 9.0761],\n",
      "        [ 8.1772],\n",
      "        [ 8.1715],\n",
      "        [ 7.9759],\n",
      "        [ 8.8481],\n",
      "        [ 8.2950],\n",
      "        [ 7.7329],\n",
      "        [ 7.7173],\n",
      "        [ 8.1834],\n",
      "        [ 7.6403],\n",
      "        [ 8.4616],\n",
      "        [ 8.7825],\n",
      "        [ 7.9151],\n",
      "        [ 8.3433],\n",
      "        [ 8.7244],\n",
      "        [ 8.3672],\n",
      "        [ 7.2627],\n",
      "        [ 8.0536],\n",
      "        [ 7.0403],\n",
      "        [ 7.7639],\n",
      "        [ 8.2321],\n",
      "        [ 7.6958],\n",
      "        [ 7.9348],\n",
      "        [ 7.8319],\n",
      "        [ 7.6772],\n",
      "        [ 7.7941],\n",
      "        [ 7.9838],\n",
      "        [ 7.4816],\n",
      "        [ 8.0182],\n",
      "        [ 7.7989],\n",
      "        [ 8.3844],\n",
      "        [ 7.4332],\n",
      "        [ 7.6735],\n",
      "        [ 7.2303],\n",
      "        [ 7.4132],\n",
      "        [ 7.9104],\n",
      "        [ 7.8874],\n",
      "        [ 7.4631],\n",
      "        [ 8.2084],\n",
      "        [ 7.1818],\n",
      "        [ 7.6292],\n",
      "        [ 8.3286],\n",
      "        [ 7.4297],\n",
      "        [ 7.6277],\n",
      "        [ 7.3633],\n",
      "        [ 7.5527],\n",
      "        [ 6.9591],\n",
      "        [ 7.5313],\n",
      "        [ 7.1240],\n",
      "        [ 7.5198],\n",
      "        [ 7.1477],\n",
      "        [ 6.9938],\n",
      "        [ 6.7993],\n",
      "        [ 7.9228],\n",
      "        [ 7.7294],\n",
      "        [ 8.1719],\n",
      "        [ 7.4151],\n",
      "        [ 6.7884],\n",
      "        [ 7.2104],\n",
      "        [ 7.3969],\n",
      "        [ 7.5783],\n",
      "        [ 7.1447],\n",
      "        [ 7.3941],\n",
      "        [ 6.9078],\n",
      "        [ 7.0395],\n",
      "        [ 6.8352],\n",
      "        [ 7.4121],\n",
      "        [ 6.3258],\n",
      "        [ 7.2616],\n",
      "        [ 7.0280],\n",
      "        [ 7.1006],\n",
      "        [ 6.8563],\n",
      "        [ 7.0640],\n",
      "        [ 6.7395],\n",
      "        [ 7.3117],\n",
      "        [ 6.9304],\n",
      "        [ 6.9456],\n",
      "        [ 6.4346],\n",
      "        [ 7.6794],\n",
      "        [ 6.6338],\n",
      "        [ 6.7028],\n",
      "        [ 6.2137],\n",
      "        [ 6.0308],\n",
      "        [ 7.2804],\n",
      "        [ 6.8215],\n",
      "        [ 5.8129],\n",
      "        [ 6.7945],\n",
      "        [ 6.7591],\n",
      "        [ 6.5191],\n",
      "        [ 6.3423],\n",
      "        [ 6.1212],\n",
      "        [ 6.0742],\n",
      "        [ 6.6252],\n",
      "        [ 6.2326],\n",
      "        [ 6.1404],\n",
      "        [ 6.5661],\n",
      "        [ 6.4583],\n",
      "        [ 6.4144],\n",
      "        [ 6.1130],\n",
      "        [ 6.5050],\n",
      "        [ 6.4818],\n",
      "        [ 5.7338],\n",
      "        [ 5.8818],\n",
      "        [ 6.1686],\n",
      "        [ 6.1661],\n",
      "        [ 6.3497],\n",
      "        [ 5.8555],\n",
      "        [ 6.4556],\n",
      "        [ 6.4736],\n",
      "        [ 5.3118],\n",
      "        [ 5.8195],\n",
      "        [ 6.1111],\n",
      "        [ 6.5072],\n",
      "        [ 6.3726],\n",
      "        [ 6.0756],\n",
      "        [ 5.6766],\n",
      "        [ 5.5083],\n",
      "        [ 6.0310],\n",
      "        [ 5.9316],\n",
      "        [ 5.9996],\n",
      "        [ 6.1166],\n",
      "        [ 5.9781],\n",
      "        [ 5.8733],\n",
      "        [ 5.6567],\n",
      "        [ 5.7597],\n",
      "        [ 5.6748],\n",
      "        [ 5.8973],\n",
      "        [ 5.9613],\n",
      "        [ 6.2374],\n",
      "        [ 5.9923],\n",
      "        [ 5.5025],\n",
      "        [ 6.0122],\n",
      "        [ 5.6706],\n",
      "        [ 5.9968],\n",
      "        [ 5.6612],\n",
      "        [ 5.3520],\n",
      "        [ 5.2728],\n",
      "        [ 5.4217],\n",
      "        [ 5.9359],\n",
      "        [ 5.6398],\n",
      "        [ 6.0634],\n",
      "        [ 5.7190],\n",
      "        [ 5.6969],\n",
      "        [ 5.8967],\n",
      "        [ 5.6408],\n",
      "        [ 5.2832],\n",
      "        [ 5.7667],\n",
      "        [ 6.4474],\n",
      "        [ 5.4848],\n",
      "        [ 6.0867],\n",
      "        [ 6.1362],\n",
      "        [ 5.8768],\n",
      "        [ 6.5500],\n",
      "        [ 6.0299],\n",
      "        [ 5.8264],\n",
      "        [ 5.1772],\n",
      "        [ 5.2451],\n",
      "        [ 5.4263],\n",
      "        [ 5.5366],\n",
      "        [ 5.4949],\n",
      "        [ 5.0766],\n",
      "        [ 6.2501],\n",
      "        [ 4.9860],\n",
      "        [ 5.2329],\n",
      "        [ 5.1542],\n",
      "        [ 5.4654],\n",
      "        [ 5.0181],\n",
      "        [ 5.1821],\n",
      "        [ 5.6204],\n",
      "        [ 5.4792],\n",
      "        [ 5.2971],\n",
      "        [ 5.2994],\n",
      "        [ 4.9845],\n",
      "        [ 5.3909],\n",
      "        [ 5.4206],\n",
      "        [ 5.6774],\n",
      "        [ 5.2282],\n",
      "        [ 5.1912],\n",
      "        [ 4.9260],\n",
      "        [ 5.1664],\n",
      "        [ 5.0531],\n",
      "        [ 5.5934],\n",
      "        [ 5.3678],\n",
      "        [ 4.9417],\n",
      "        [ 4.9851],\n",
      "        [ 4.9637],\n",
      "        [ 5.0860],\n",
      "        [ 4.9264],\n",
      "        [ 5.0251],\n",
      "        [ 4.9417],\n",
      "        [ 5.1355],\n",
      "        [ 4.1949],\n",
      "        [ 5.0124],\n",
      "        [ 4.5859],\n",
      "        [ 5.0862],\n",
      "        [ 4.5982],\n",
      "        [ 4.6908],\n",
      "        [ 4.8074],\n",
      "        [ 4.8976],\n",
      "        [ 4.8070],\n",
      "        [ 4.8618],\n",
      "        [ 4.6690],\n",
      "        [ 5.0532],\n",
      "        [ 4.6505],\n",
      "        [ 4.8112],\n",
      "        [ 4.6232],\n",
      "        [ 4.1234],\n",
      "        [ 4.6767],\n",
      "        [ 4.4554],\n",
      "        [ 5.1396],\n",
      "        [ 4.7448],\n",
      "        [ 4.5001],\n",
      "        [ 4.3694],\n",
      "        [ 4.4883],\n",
      "        [ 4.4489],\n",
      "        [ 4.3065],\n",
      "        [ 4.3386],\n",
      "        [ 4.5020],\n",
      "        [ 4.5539],\n",
      "        [ 4.2377],\n",
      "        [ 4.2371],\n",
      "        [ 4.2143],\n",
      "        [ 4.2964],\n",
      "        [ 4.4235],\n",
      "        [ 4.1981],\n",
      "        [ 4.2088],\n",
      "        [ 4.1312],\n",
      "        [ 4.0998],\n",
      "        [ 4.5238],\n",
      "        [ 3.9176],\n",
      "        [ 4.0914],\n",
      "        [ 3.7720],\n",
      "        [ 4.0840],\n",
      "        [ 3.9131],\n",
      "        [ 3.7081],\n",
      "        [ 3.9010],\n",
      "        [ 4.0216],\n",
      "        [ 3.7595],\n",
      "        [ 3.7778],\n",
      "        [ 3.4758],\n",
      "        [ 3.9256],\n",
      "        [ 3.8454],\n",
      "        [ 3.5121],\n",
      "        [ 3.6674],\n",
      "        [ 3.6541],\n",
      "        [ 3.4280],\n",
      "        [ 3.6862],\n",
      "        [ 3.8986],\n",
      "        [ 3.6642],\n",
      "        [ 3.5593],\n",
      "        [ 3.5948],\n",
      "        [ 3.8086],\n",
      "        [ 3.5398],\n",
      "        [ 3.3638],\n",
      "        [ 3.3687],\n",
      "        [ 3.2543],\n",
      "        [ 3.4394],\n",
      "        [ 3.3286],\n",
      "        [ 3.2932],\n",
      "        [ 3.2354],\n",
      "        [ 3.2199],\n",
      "        [ 3.2611],\n",
      "        [ 3.3994],\n",
      "        [ 3.0158],\n",
      "        [ 3.2122],\n",
      "        [ 3.0269],\n",
      "        [ 3.0396],\n",
      "        [ 2.7105],\n",
      "        [ 2.9152],\n",
      "        [ 2.8116],\n",
      "        [ 2.8462],\n",
      "        [ 2.9581],\n",
      "        [ 2.9747],\n",
      "        [ 2.6615],\n",
      "        [ 2.7692],\n",
      "        [ 2.7103],\n",
      "        [ 2.5225],\n",
      "        [ 2.4555],\n",
      "        [ 2.5238],\n",
      "        [ 2.4583],\n",
      "        [ 2.4564],\n",
      "        [ 2.4378],\n",
      "        [ 2.3594],\n",
      "        [ 2.3240],\n",
      "        [ 2.2792],\n",
      "        [ 1.9962],\n",
      "        [ 2.1629],\n",
      "        [ 2.0261],\n",
      "        [ 1.9703],\n",
      "        [ 2.3539],\n",
      "        [ 1.8160],\n",
      "        [ 1.6803],\n",
      "        [ 1.8129],\n",
      "        [ 1.6945],\n",
      "        [ 1.7267],\n",
      "        [ 1.7723],\n",
      "        [ 1.5667],\n",
      "        [ 1.4688],\n",
      "        [ 1.2756],\n",
      "        [ 0.7513]])\n"
     ]
    }
   ],
   "source": [
    "# se utilizar谩 un perceptr贸n que va permitir aproximar el modelo por medio de descenso del gradiente\n",
    "\n",
    "n_epochs = 50\n",
    "w = torch.rand(6,1)\n",
    "\n",
    "# se define el learning rate\n",
    "lr = 0.1\n",
    "\n",
    "SamplesAll = SamplesAll.to(torch.float32)\n",
    "w = w.to(torch.float32)\n",
    "\n",
    "# por cada iteraci贸n se actualiza el valor de pesos\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = SamplesAll.mm(w)\n",
    "\n",
    "    error = targetsAll - yhat\n",
    "\n",
    "    # la actualizaci贸n depende del gradiente del error, entre m谩s cercano a cero puede \n",
    "    #    actualizar el valor\n",
    "    w_grad = -2  * error.mean()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w_grad\n",
    "\n",
    "# Se imprimen los resultados para comparar\n",
    "\n",
    "print(yhat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Realice al menos tres conclusiones sobre el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La aproximaci贸n manual del modelo requiere de m谩s c贸digo que la que utiliza los m贸dulos de pytorch.\n",
    "- Otra opci贸n para realizar regresi贸n corresponde a la t茅cnica de descenso de gradiente.\n",
    "- La forma de comprobar si el modelo est谩 bien entrenado corresponde a a帽adir valores de entrada conocidos y utilizar el perceptr贸n para definir si el resultado estimado se acerca al real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias**\n",
    "\n",
    "[1] 'Exploraci贸n y vusualizaci贸n de datos', notas de clase del curso IC-6200, Sede Interuniversitaria de Alajuela, I Semestre del 2022.\n",
    "\n",
    "[2] 'Regresi贸n no lineal', notas de clase del curso IC-6200, Sede Interuniversitaria de Alajuela, I Semestre del 2022.\n",
    "\n",
    "[3] 'Regresi贸n lineal', notas de clase del curso IC-6200, Sede Interuniversitaria de Alajuela, I Semestre del 2022.\n",
    "\n",
    "[4] Pytorch (2022) Pythorch Documentation. Disponible en: https://pytorch.org/docs/stable/index.html\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60900aa771768904a6d6616df638e60e3e0400762e431fc6f8b11a1c64e30e12"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
